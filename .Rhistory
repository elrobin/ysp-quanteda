length(year_lines)
# Unir todas las líneas de la entrada diaria en un solo texto
paste(year_lines[start_day:end_day], collapse = " ")
})
# Devolver una lista con el año y sus entradas diarias
list(year = year_lines[1], entries = entries)
})
print(yearly_entries[[1]]$year) # primer año
length(yearly_entries[[1]]$entries)
# Asignar nombres únicos a cada entrada en yearly_entries
yearly_entries <- lapply(seq_along(yearly_entries), function(i) {
year <- yearly_entries[[i]]
# Asignar nombres únicos a cada entrada diaria
names(year$entries) <- paste0("entry_", i, "_", seq_along(year$entries))
year  # Devolver el año con las entradas renombradas
})
# Verificar los nombres de las entradas en el primer año
print(names(yearly_entries[[1]]$entries))
# Convertir cada entrada diaria a minúsculas y eliminar signos de puntuación
yearly_entries <- lapply(yearly_entries, function(year) {
year$entries <- lapply(year$entries, function(entry) {
# Convertir el texto a minúsculas y eliminar puntuación
entry <- char_tolower(entry)
entry <- gsub("[[:punct:]]", "", entry)
entry
})
year  # Devolver la lista de año modificada
})
# Aquí podemos ver qué ha pasado con las primeras 5 líneas de la primera entrada
print(head(yearly_entries[[1]]$entries[[1]], 5))
first_entry_words <- unlist(strsplit(yearly_entries[[1]]$entries[[1]], " "))
print(head(first_entry_words, 10))
# Dividir la primera entrada en palabras, seleccionar las primeras 10 y unirlas en una sola cadena de texto
first_entry_preview <- paste(head(unlist(strsplit(yearly_entries[[1]]$entries[[1]], " ")), 10), collapse = " ")
print(first_entry_preview)
# Tokenizar cada entrada diaria dentro de cada año
yearly_entries <- lapply(yearly_entries, function(year) {
year$entries <- lapply(year$entries, function(entry) {
tokens(entry, what = "word")
})
year  # Devolver la lista de año modificada
})
print(head(yearly_entries[[1]]$entries[[1]], 3))
# Eliminar palabras vacías en inglés en cada entrada diaria
yearly_entries <- lapply(yearly_entries, function(year) {
year$entries <- lapply(year$entries, function(entry) {
tokens_remove(entry, pattern = stopwords("en"))
})
year  # Devolver la lista de año modificada
})
print(head(yearly_entries[[1]]$entries[[1]], 3))
# Crear una matriz de frecuencias de términos a partir de los tokens ya generados
dfm_all <- do.call(rbind, lapply(yearly_entries, function(year) {
# Convertir las entradas diarias a tokens antes de crear la matriz de frecuencias
tokens_year <- tokens(do.call(c, year$entries))
dfm(tokens_year)
}))
# Dividimos el texto en entradas anidadas (por año y día)
yearly_entries <- lapply(
seq_along(year_indices),
function(i) {
start_year <- year_indices[i]
end_year <- if (i < length(year_indices))
year_indices[i + 1] - 1
else
length(lines)
# Extraemos las líneas correspondientes al año actual
year_lines <- lines[start_year:end_year]
# Encontrar las entradas diarias dentro del año actual
day_indices <- grep(
"^(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(st|nd|rd|th)?$",
year_lines
)
# Crear una sublista para cada día dentro del año
entries <- sapply(seq_along(day_indices), function(j) {
start_day <- day_indices[j]
end_day <- if (j < length(day_indices))
day_indices[j + 1] - 1
else
length(year_lines)
# Unir todas las líneas de la entrada diaria en un solo texto
paste(year_lines[start_day:end_day], collapse = " ")
}, simplify = TRUE)
# Devolver una lista con el año y sus entradas diarias
list(year = year_lines[1], entries = entries)
})
yearly_entries[[1]][["entries"]]
View[yearly_entries[[1]][["entries"]]]
yearly_entries[[1]][["entries"]][1]
yearly_entries[[1]][["entries"]][2]
yearly_entries[[1]][["entries"]][3]
# Asignar nombres únicos a cada entrada en yearly_entries
yearly_entries <- lapply(seq_along(yearly_entries), function(i) {
year <- yearly_entries[[i]]
# Asignar nombres únicos a cada entrada diaria
names(year$entries) <- paste0("entry_", i, "_", seq_along(year$entries))
year  # Devolver el año con las entradas renombradas
})
# Verificar los nombres de las entradas en el primer año
print(names(yearly_entries[[1]]$entries))
yearly_entries[[1]][["entries"]]$entry_1_1
# Convertir cada entrada diaria a minúsculas y eliminar signos de puntuación
yearly_entries <- lapply(yearly_entries, function(year) {
year$entries <- lapply(year$entries, function(entry) {
# Convertir el texto a minúsculas y eliminar puntuación
entry <- char_tolower(entry)
entry <- gsub("[[:punct:]]", "", entry)
entry
})
year  # Devolver la lista de año modificada
})
# Dividir la primera entrada en palabras, seleccionar las primeras 10 y unirlas en una sola cadena de texto
first_entry_preview <- paste(head(unlist(strsplit(yearly_entries[[1]]$entries[[1]], " ")), 10), collapse = " ")
print(first_entry_preview)
# Tokenizar cada entrada diaria dentro de cada año
yearly_entries <- lapply(yearly_entries, function(year) {
year$entries <- lapply(year$entries, function(entry) {
tokens(entry, what = "word")
})
year  # Devolver la lista de año modificada
})
print(head(yearly_entries[[1]]$entries[[1]], 3))
# Eliminar palabras vacías en inglés en cada entrada diaria
yearly_entries <- lapply(yearly_entries, function(year) {
year$entries <- lapply(year$entries, function(entry) {
tokens_remove(entry, pattern = stopwords("en"))
})
year  # Devolver la lista de año modificada
})
print(head(yearly_entries[[1]]$entries[[1]], 3))
# Crear una matriz de frecuencias de términos a partir de los tokens ya generados
dfm_all <- do.call(rbind, lapply(yearly_entries, function(year) {
# Convertir las entradas diarias a tokens antes de crear la matriz de frecuencias
tokens_year <- tokens(do.call(c, year$entries))
dfm(tokens_year)
}))
# Extraer el texto de cada entrada diaria y crear una lista plana
unified_entries <- unlist(lapply(yearly_entries, function(year) {
sapply(year$entries, function(entry) paste(entry, collapse = " "))
}))
# Crear una matriz de frecuencias de términos a partir de los textos unificados
dfm_all <- dfm(tokens(unified_entries))
# Mostrar las 10 palabras más frecuentes
top_words <- topfeatures(dfm_all, n = 10)
top_words
print(dfm_all)
print(head(dfm_all))
print(head(dfm_all))
save.image(file = ".RData")
loac(".RData")
load(".RData")
library(quanteda)
library(readtext)
library(dplyr)
library(stringi)
saveRDS(yearly_entries, file = "yearly_entries.rds")
quarto render dataviz.qmd
# Renderiza el proyecto Quarto
quarto::quarto_render()
# Define el directorio de salida y el directorio de destino (raíz)
output_dir <- "_site"
main_dir <- "."
# Mueve toda la carpeta `_site` a la raíz
file.rename(output_dir, main_dir)
# Extraer el texto de cada entrada diaria y crear una lista plana
unified_entries <- unlist(lapply(yearly_entries, function(year) {   sapply(year$entries, function(entry) paste(entry, collapse = " ")) }))
# Crear una matriz de frecuencias de términos a partir de los textos unificados
dfm_all <- dfm(tokens(unified_entries))
# Mostrar las 10 palabras más frecuentes
top_words <- topfeatures(dfm_all, n = 10)
top_words
load(".RData")
library(quanteda)
library(readtext)
library(dplyr)
library(stringi)
# Extraer el texto de cada entrada diaria y crear una lista plana
unified_entries <- unlist(lapply(yearly_entries, function(year) {   sapply(year$entries, function(entry) paste(entry, collapse = " ")) }))
# Crear una matriz de frecuencias de términos a partir de los textos unificados
dfm_all <- dfm(tokens(unified_entries))
# Mostrar las 10 palabras más frecuentes
top_words <- topfeatures(dfm_all, n = 10)
top_words
print(head(dfm_all))
print(head(dfm_all))
library(ggplot2)
install.packages("ggplot2")
library(ggplot2)
# Convertir el top 10 en un dataframe
top_words_df <- data.frame(
word = names(top_words),
frequency = as.numeric(top_words)
)
# Crear el gráfico de barras
ggplot(top_words_df, aes(x = reorder(word, frequency), y = frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
labs(title = "Top 10 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme_minimal()
# Librería necesaria
# library(quanteda.textplots)
# Palabras clave para el análisis de dispersión léxica
keywords <- c("cell", "russia", "people", "putin")
# Crear los tokens a partir del texto unificado
tokens_all <- tokens(unified_entries)
# Generar el gráfico de dispersión léxica
textplot_xray(kwic(tokens_all, pattern = keywords))
# Generar el gráfico de dispersión léxica
quanteda.textplots::textplot_xray(kwic(tokens_all, pattern = keywords))
# Librería necesaria
library(quanteda.textplots)
# Generar el gráfico de dispersión léxica
textplot_xray(kwic(tokens_all, pattern = keywords))
install.packages("quanteda.textplots")
# Librería necesaria
library(quanteda.textplots)
# Palabras clave para el análisis de dispersión léxica
keywords <- c("cell", "russia", "people", "putin")
# Crear los tokens a partir del texto unificado
tokens_all <- tokens(unified_entries)
# Generar el gráfico de dispersión léxica
textplot_xray(kwic(tokens_all, pattern = keywords))
# Unificar todas las entradas en un solo texto
full_text <- paste(unified_entries, collapse = " ")
# Crear tokens a partir del texto completo
tokens_full_text <- tokens(full_text)
# Generar gráfico de dispersión léxica para todo el texto
textplot_xray(kwic(tokens_full_text, pattern = keywords))
# Librería necesaria
library(quanteda.textplots)
# Palabras clave para el análisis de dispersión léxica
keywords <- c("cell", "russia", "people", "death")
# Crear los tokens a partir del texto unificado
tokens_all <- tokens(unified_entries)
# Generar el gráfico de dispersión léxica
textplot_xray(kwic(tokens_all, pattern = keywords))
# Librería necesaria
library(quanteda.textplots)
# Palabras clave para el análisis de dispersión léxica
keywords <- c("cell", "russia", "people", "putin")
# Crear los tokens a partir del texto unificado
tokens_all <- tokens(unified_entries)
# Generar el gráfico de dispersión léxica
textplot_xray(kwic(tokens_all, pattern = keywords))
# Unificar todas las entradas en un solo texto
full_text <- paste(unified_entries, collapse = " ")
# Crear tokens a partir del texto completo
tokens_full_text <- tokens(full_text)
# Generar gráfico de dispersión léxica para todo el texto
textplot_xray(kwic(tokens_full_text, pattern = keywords))
# Crear tokens a partir del texto unificado por entradas
tokens_entries <- tokens(unified_entries)
# Generar gráfico de dispersión léxica por entrada
textplot_xray(kwic(tokens_entries, pattern = keywords))
# Extraer el texto de cada entrada diaria y crear una lista plana
unified_entries <- unlist(lapply(yearly_entries, function(year) {   sapply(year$entries, function(entry) paste(entry, collapse = " ")) }))    # Crear una matriz de frecuencias de términos a partir de los textos unificados
dfm_all <- dfm(tokens(unified_entries))
# Mostrar las 10 palabras más frecuentes
top_words <- topfeatures(dfm_all, n = 20)
top_words
# Extraer el texto de cada entrada diaria y crear una lista plana
unified_entries <- unlist(lapply(yearly_entries, function(year) {   sapply(year$entries, function(entry) paste(entry, collapse = " ")) }))    # Crear una matriz de frecuencias de términos a partir de los textos unificados
dfm_all <- dfm(tokens(unified_entries))
# Mostrar las 10 palabras más frecuentes
top_words <- topfeatures(dfm_all, n = 15)
top_words
print(head(dfm_all))
library(ggplot2)
# Convertir el top 10 en un dataframe
top_words_df <- data.frame(
word = names(top_words),
frequency = as.numeric(top_words)
)
# Crear el gráfico de barras
ggplot(top_words_df, aes(x = reorder(word, frequency), y = frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
labs(title = "Top 10 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme_minimal()
library(ggplot2)
# Convertir el top 10 en un dataframe
top_words_df <- data.frame(
word = names(top_words),
frequency = as.numeric(top_words)
)
# Crear el gráfico de barras
ggplot(top_words_df, aes(x = reorder(word, frequency), y = frequency)) +
geom_bar(stat = "identity", fill = "black") +
coord_flip() +
labs(title = "Top 10 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme_minimal()
# Librería necesaria
library(quanteda.textplots)
# Palabras clave para el análisis de dispersión léxica
keywords <- c("cell", "russia", "people", "night")
# Unificar todas las entradas en un solo texto
full_text <- paste(unified_entries, collapse = " ")
# Crear tokens a partir del texto completo
tokens_full_text <- tokens(full_text)
# Generar gráfico de dispersión léxica para todo el texto
textplot_xray(kwic(tokens_full_text, pattern = keywords))
# Librería necesaria
library(quanteda.textplots)
# Palabras clave para el análisis de dispersión léxica
keywords <- c("cell", "russia", "people", "prison")
# Unificar todas las entradas en un solo texto
full_text <- paste(unified_entries, collapse = " ")
# Crear tokens a partir del texto completo
tokens_full_text <- tokens(full_text)
# Generar gráfico de dispersión léxica para todo el texto
textplot_xray(kwic(tokens_full_text, pattern = keywords))
# Librería necesaria
library(quanteda.textplots)
# Palabras clave para el análisis de dispersión léxica
keywords <- c("cell", "russia", "people", "prison")
# Unificar todas las entradas en un solo texto
full_text <- paste(unified_entries, collapse = " ")
# Crear tokens a partir del texto completo
tokens_full_text <- tokens(full_text)
# Generar gráfico de dispersión léxica para todo el texto
textplot_xray(kwic(tokens_full_text, pattern = keywords))
library(quanteda.textplots)
# Generar la nube de palabras a partir de la matriz de frecuencias
textplot_wordcloud(dfm_all, max_words = 100, color = "skyblue")
library(quanteda.textplots)
# Generar la nube de palabras a partir de la matriz de frecuencias
textplot_wordcloud(dfm_all, max_words = 100, color = "orange")
# Agrupar las entradas por año unificándolas en un solo texto por año
unified_by_year <- sapply(yearly_entries, function(year) paste(year$entries, collapse = " "))
# Crear tokens a partir del texto unificado por año
tokens_by_year <- tokens(unified_by_year)
# Generar gráfico de dispersión léxica agrupado por año
textplot_xray(kwic(tokens_by_year, pattern = keywords))
# Aquí es importante incluir palabras presentes en cada unidad de análisis (año)
keywords <- c("russia", "cell")
# Agrupar las entradas por año unificándolas en un solo texto por año
unified_by_year <- sapply(yearly_entries, function(year) paste(year$entries, collapse = " "))
# Crear tokens a partir del texto unificado por año
tokens_by_year <- tokens(unified_by_year)
# Generar gráfico de dispersión léxica agrupado por año
textplot_xray(kwic(tokens_by_year, pattern = keywords))
# Generar gráfico de dispersión léxica agrupado por año
textplot_xray(kwic(tokens_by_year, pattern = "Russia"))
# Generar gráfico de dispersión léxica agrupado por año
textplot_xray(kwic(tokens_by_year, pattern = "russia"))
# Generar gráfico de dispersión léxica agrupado por año
textplot_xray(kwic(tokens_by_year, pattern = "cell"))
# Librería necesaria
library(quanteda.textplots)
# Palabras clave para el análisis de dispersión léxica
keywords <- c("cell", "russia", "people", "prison")
# Unificar todas las entradas en un solo texto
full_text <- paste(unified_entries, collapse = " ")
# Crear tokens a partir del texto completo
tokens_full_text <- tokens(full_text)
# Generar gráfico de dispersión léxica para todo el texto
textplot_xray(kwic(tokens_full_text, pattern = keywords))
# Crear un vector vacío para almacenar las entradas por mes y año
entries_by_month <- list()
# Iterar sobre cada año en yearly_entries
for (year_data in yearly_entries) {
# Extraer el año
year <- year_data$year
# Iterar sobre cada entrada en el año
for (entry in year_data$entries) {
# Extraer el mes del comienzo de la entrada
month <- str_extract(entry, "^(January|February|March|April|May|June|July|August|September|October|November|December)")
# Crear una clave única para cada mes y año
month_year <- paste(year, month)
# Añadir la entrada al grupo correspondiente
if (!is.null(month) && month != "") {
if (!month_year %in% names(entries_by_month)) {
entries_by_month[[month_year]] <- ""
}
entries_by_month[[month_year]] <- paste(entries_by_month[[month_year]], entry, collapse = " ")
}
}
}
library(stringr)
# Crear un vector vacío para almacenar las entradas por mes y año
entries_by_month <- list()
# Iterar sobre cada año en yearly_entries
for (year_data in yearly_entries) {
# Extraer el año
year <- year_data$year
# Iterar sobre cada entrada en el año
for (entry in year_data$entries) {
# Extraer el mes del comienzo de la entrada
month <- str_extract(entry, "^(January|February|March|April|May|June|July|August|September|October|November|December)")
# Crear una clave única para cada mes y año
month_year <- paste(year, month)
# Añadir la entrada al grupo correspondiente
if (!is.null(month) && month != "") {
if (!month_year %in% names(entries_by_month)) {
entries_by_month[[month_year]] <- ""
}
entries_by_month[[month_year]] <- paste(entries_by_month[[month_year]], entry, collapse = " ")
}
}
}
# Asegúrate de que el paquete stringr está cargado
library(stringr)
# Crear un vector vacío para almacenar las entradas por mes y año
entries_by_month <- list()
# Iterar sobre cada año en yearly_entries
for (year_data in yearly_entries) {
# Extraer el año
year <- year_data$year
# Iterar sobre cada entrada en el año
for (entry in year_data$entries) {
# Extraer el mes del comienzo de la entrada sin tokenizar
month <- str_extract(entry, "^(January|February|March|April|May|June|July|August|September|October|November|December)")
# Crear una clave única para cada mes y año
month_year <- paste(year, month)
# Añadir la entrada al grupo correspondiente
if (!is.null(month) && month != "") {
if (!month_year %in% names(entries_by_month)) {
entries_by_month[[month_year]] <- ""
}
entries_by_month[[month_year]] <- paste(entries_by_month[[month_year]], entry, collapse = " ")
}
}
}
# Definir las palabras clave de interés
keywords <- c("cell", "russia", "people", "prison")
# Crear tokens a partir de todas las entradas unificadas
tokens_all <- tokens(unified_entries)
# Extraer concordancias de las palabras clave
concordancias <- kwic(tokens_all, pattern = keywords, window = 5)
print(concordancias)
# Crear tokens a partir de todas las entradas unificadas
tokens_all <- tokens(unified_entries)
# Extraer concordancias de las palabras clave
concordancias <- kwic(tokens_all, pattern = "russia", window = 5)
print(concordancias)
# Extraer concordancias de las palabras clave
concordancias <- kwic(tokens_full_text, pattern = "russia", window = 5)
# Extraer concordancias de las palabras clave
concordancias <- kwic(tokens_full_text, pattern = "russia", window = 5)
print(concordancias)
# Extraer concordancias de las palabras clave
concordancias <- kwic(tokens_full_text, pattern = "russia", window = 2)
print(concordancias)
# Extraer concordancias de las palabras clave
concordancias <- kwic(tokens_full_text, pattern = "shizo", window = 2)
print(concordancias)
# Librería necesaria
library(quanteda.textplots)
# Palabras clave para el análisis de dispersión léxica
keywords <- c("cell", "russia", "people", "shizo")
# Unificar todas las entradas en un solo texto
full_text <- paste(unified_entries, collapse = " ")
# Crear tokens a partir del texto completo
tokens_full_text <- tokens(full_text)
# Generar gráfico de dispersión léxica para todo el texto
textplot_xray(kwic(tokens_full_text, pattern = keywords))
# Convertir las concordancias en un data frame
concordancias_df <- as.data.frame(concordancias)
# Mostrar la tabla de concordancias
print(concordancias_df)
dfm_topics <- dfm_trim(dfm_all, min_termfreq = 5, max_docfreq = 0.8)
length(dfm_all)
length(dfm_topics) # total una vez eliminados los poco frecuentes
length(dfm_all) # total de términos
length(dfm_topics) # total una vez eliminados los poco frecuentes
dfm_topics <- dfm_trim(dfm_all, min_termfreq = 5)
length(dfm_all) # total de términos
length(dfm_topics) # total una vez eliminados los poco frecuentes
# Convertir la matriz dfm a formato compatible con LDA
dfm_topics_lda <- convert(dfm_topics, to = "topicmodels")
install.packages("tm")
# Convertir la matriz dfm a formato compatible con LDA
dfm_topics_lda <- convert(dfm_topics, to = "topicmodels")
# Definir el número de temas
num_topics <- 5  # Ajusta el número de temas según el análisis deseado
# Ajustar el modelo LDA
lda_model <- LDA(dfm_topics_lda, k = num_topics, control = list(seed = 1234))
install.packages("topicmodels")
library(tm)
library(topicmodels)
# Convertir la matriz dfm a formato compatible con LDA
dfm_topics_lda <- convert(dfm_topics, to = "topicmodels")
# Definir el número de temas
num_topics <- 5  # Ajusta el número de temas según el análisis deseado
# Ajustar el modelo LDA
lda_model <- LDA(dfm_topics_lda, k = num_topics, control = list(seed = 1234))
library(tm)
library(topicmodels)
# Convertir la matriz dfm a formato compatible con LDA
dfm_topics_lda <- convert(dfm_topics, to = "topicmodels")
# Definir el número de temas
num_topics <- 5  # Ajusta el número de temas según el análisis deseado
# Ajustar el modelo LDA
lda_model <- LDA(dfm_topics_lda, k = num_topics, control = list(seed = 1234))
# Obtener las palabras clave de cada tema
terms(lda_model, 10)  # Muestra las 10 palabras más representativas de cada tema
dfm_topics
# Definir el número de temas
num_topics <- 3  # Ajusta el número de temas según el análisis deseado
# Ajustar el modelo LDA
lda_model <- LDA(dfm_topics_lda, k = num_topics, control = list(seed = 1234))
# Obtener las palabras clave de cada tema
terms(lda_model, 10)  # Muestra las 10 palabras más representativas de cada tema
install.packages("topicmodels")
View(yearly_entries)
View(yearly_entries)
