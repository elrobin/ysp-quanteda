---
title: "Carga y procesamiento de textos"
format: html
toc: true   
toc-title: "√çndice"       
toc-depth: 4              # Por alguna raz√≥n no muestra t√≠tulos de nivel 4. Revisarlo
code-copy: true
editor: visual
---

## üì¶Preparando los paquetes

Para seguir este tutorial por primera vez, deber√°s instalar una serie de paquetes[^1] que emplearemos: `quanteda`, `readtext`, `dplyr` y `stringr`. Esto se hace a trav√©s del comando: `install.packages()`:

[^1]:
    ::: {.callout-tip appearance="simple"}
    `R` es un lenguaje de programaci√≥n abierto y colaborativo que sigue una estructura totalmente descentralizada. Cuando instalamos `R` por primera vez, s√≥lo instalamos sus funcionalidades b√°sicas. Todas aquellas funcionalidades adicionales llevadas a cabo por terceras personas deben instalarse en lo que se denominan **paquetes** o **libraries** en ingl√©s.

    Cada vez que vayas a emplear un paquete, debes cargarlo, por defecto, cada vez que abres `R` estos paquetes no est√°n cargados.
    :::

```{r, eval = FALSE}
install.packages(c("quanteda", "readtext", "dplyr", "stringi"))
```

#### ¬øPor qu√© lo usamos?

`quanteda`

:   Paquete de an√°lisis de textos, incluyendo tokenizaci√≥n, conteo y limpieza de textos

`readtext`

:   Permite importar archivos de texto en varios formatos, facilitando la carga de datos

`dplyr`

:   Herramienta para manipulaci√≥n y transformaci√≥n de datos, √∫til para filtrar y organizar datos

`stringi`

:   Conjunto de funciones para trabajar con texto, especialmente √∫til para limpieza y manejo de expresiones regulares.[^2]

[^2]: Una **expresi√≥n regular** es un patr√≥n de b√∫squeda utilizado para manipular texto espec√≠fico en una cadena. Facilita tareas como eliminar caracteres no deseados o extraer informaci√≥n espec√≠fica (e.g., fechas o n√∫meros). Es especialmente √∫til en la limpieza y preprocesamiento de datos textuales.

**¬øSab√≠as qu√©...?** Ô∏èü§ì‚òù

::: {.tip style="background-color: #A8E6A1; border-left: 5px solid #A8E6A1; padding: 10px"}
La **tokenizaci√≥n** es el proceso de dividir un texto en unidades m√°s peque√±as llamdas *tokens*. Estas unidades pueden ser palabras, s√≠mbolos, frases o incluso caracteres dependiendo del tipo de an√°lisis que se vaya a realizar. En `quanteda` consideramos la palabra como la unidad m√≠nima de trabajo. Imag√≠nate que tienes la siguiente oraci√≥n:

"Hola, ¬øc√≥mo est√°s?"

La tokenizaci√≥n de esta frase podr√≠a dar como resultado los siguientes tokens: "Hola", "¬ø", "c√≥mo", "est√°s", "?"

Esto es especialmente √∫til cuando estamos trabajando con estudios relacionados con frecuencias de palabras.
:::

## üìÑ‚û°Ô∏èüñ•Ô∏èImportaci√≥n de datos de texto

Vamos a vincular el archivo de¬†texto en la aplicaci√≥n de RStudios. Para ello vamos a cargar los paquetes que hemos instalado anteriormente¬†con el comando `library("name")`

```{r, eval=T}
library(quanteda)
library(readtext)
library(dplyr)
library(stringi)
```

Es importante resaltar que, si no llamamos antes el paquete, los comandos que introduzcamos despu√©s no funcionar√°n o nos dar√°n error. Aseg√∫rate de cargar siempre la librer√≠a antes de empezar a trabajar.

Una vez cargados, el programa estar√° listo para leer nuestro archivo de texto. La formula que vamos a escribir para decirle a `quanteda` que archivo analizar ser√° el siguiente:

```{r, eval=T}
navalny_raw <- as.character(readtext("NAVALNY.txt"))
names(navalny_raw) <- "navalny"
```

**‚ùóATENCI√ìN:** Si por alguna raz√≥n hiciesemos alg√∫n cambio en el contenido del archivo, deberemos de aplicar el paso anterior de nuevo. Cuando cargamos un archivo en R, se guarda una copia y cualquier cambio en el original no se refleja autom√°ticamente.

#### ü§î¬øQu√© hemos hecho?

Este comando carga el archivo `NAVALNY.txt` en el objeto `navalny_raw`, el cual contiene el contenido del texto. Vamos a desgranar este prompt para que pueda entenderse m√°s facil:

-   `navalny_raw` es un objeto en R que almacena el texto como una cadena de caracteres (character vector). En `R` hablamos de **objetos** para referirnos a los contenedores donde almacenamos datos e informaci√≥n. En el caso anterior, el objeto `data_char_navalny` almacena el texto plano que vamos a utilizar. Existen distintos objetos con diferentes datos almacenados: matrices, n√∫meros, listas jerarquizadas, as√≠ como un sin fin de combinaciones. A lo largo de este caso pr√°ctico trabajaremos con ellos para gestionar m√°s facilmente el an√°lisis cuantitativo.

-   `<-` emula a una flecha y b√°sicamente indica la direcci√≥n de la acci√≥n. Al objeto `navalny_raw` estamos aplic√°ndole una funci√≥n

-   `as.character()`: Se trata de la funci√≥n que estamos aplicando. Esta funci√≥n convierte todo lo que se contiene dentro de ella en car√°cter. En `R` toda funci√≥n viene seguida por unos par√©ntesis dentro de los cu√°les se incluyen los par√°metros de dicha funci√≥n. Si no hay contenido, se aplican los par√°metros que la funci√≥n trae por defecto.

-   `readtext`: Aqu√≠ estamos empleando una funci√≥n expec√≠fica del paquete `readtext`. Si no hubi√©semos cargado el paquete anteriormente, podr√≠amos invocarlo espec√≠ficamente para activar esta funci√≥n con la notaci√≥n `paquete::funci√≥n`. Aqu√≠, lo que le estamos diciendo a RStudio es que, del paquete `readtext`, aplique espec√≠ficamente la funci√≥n lectura que casualmente tambi√©n se llama `readtext` . Dentro indicamos entrecomillada la ruta del archivo a importar. Un **problema muy com√∫n** que puede surgir a la hora de introducir la URL de la ubicaci√≥n del archivo es expresarlo con barras laterales izquierdas " \\ ", tal y como viene en la barra de direcci√≥n del explorador de archivos de Windows, en vez de la derecha " / ". Si tienes problemas leer tu .txt ¬°prueba con hacer este cambio!

-   `names(navalny_raw) <- "navalny"`: Asigna un nombre al objeto que contiene el texto, facilitando su identificaci√≥n en futuros an√°lisis.

#### üîç Comprobaciones

En el an√°lisis cuantitativo toda precauci√≥n es poca. Vamos a verificar que el texto ha sido le√≠do por el programa. Usaremos el paquete `stringi` para ver los primeros 75 caracteres de nuestro archivo y confirmar que los datos se cargaron bien.

```{R, eval=T}

# Comprobar los primeros 75 caracteres del texto
stri_sub(navalny_raw, 1, 75)
```

Si hemos hecho los pasos bien, tendr√©is que haber recibir este texto de vuelta:

```         
[1] "2022\nJanuary 17th\nExactly one year ago today I came home, to Russia.\nI didn"
```

## üóÉÔ∏èAcotando el texto a analizar

Nuestro siguiente objetivo es seleccionar qu√© partes del texto vamos aplicar el an√°lisis cuantitativo. Puede que nuestro foco de inter√©s sea algun apartado concreto de nuestro material, por lo que vamos a crear un objeto que albergue un rango determinado dentro de nuestro fichero `txt` . Con esto, nos quitaremos toda la informaci√≥n innecesaria que puede ensuciar nuestros resultados.

El proceso que vamos a realizar a continuaci√≥n es muy √∫til cuando los archivos que manejamos tienen ligados metadatos. Normalmente, esta metadata suele ser m√°s un dolor de cabeza que otra cosa y es recomendable realizar una limpieza previa para que esos datos no se mezclen con el contenido de nuestro an√°lisis. En este apartado seguiremos trabajando con el paquete `stringi`.

Si el texto contiene secciones que no necesitamos para el an√°lisis, podemos filtrarlas o limpiarlas en esta etapa.

#### PASO 1: Identificaci√≥n de comienzo y fin del texto

Para crear el objeto que albergue el rango de texto a analizar deberemos empezar indicando donde empieza y termina nuestra selecci√≥n. Para ello, crearemos dos valores de posici√≥n: `start_v` y `end_v`, donde `start_` ser√°: "2023, January 12th" y `end_v` "Alexei Navalny died".

Localizado el rango que queremos, la forma de expresarlo en el programa ser√≠a el siguiente:

```{r, eval=T}
(start_v <- stri_locate_first_fixed(navalny_raw, "2023\nJanuary 12th")[1])
```

```{r, eval=T}
(end_v <- stri_locate_last_fixed(navalny_raw, "Alexei Navalny died")[1])
```

Si lo hemos aplicado bien, la funci√≥n deber√≠a de devolver los siguientes resultados

-   Para `start_value`

    ```         
    [1] 23653
    ```

<!-- -->

-   Para `end_value`

    ```         
    [1] 44099
    ```

#### ü§î¬øQu√© hemos hecho?

-   Tanto `start_v` como `end_v` son nombres que hemos asignado a la posici√≥n espec√≠ficas del texto. En s√≠, no significan nada. Solo decimos, a trav√©s de "\<- " que dichos nombres albergan una funci√≥n de posicionamiento.

-   Las funciones del paquete `stringi`: `stri_locate_first_fixed` y `stri_locate_last_fixed` buscan y encuentran la primera coincidencia del valor entrecomillado que precede a nuestro objeto `data_char_NALVANY`

-   El \[1\] es un indicador que le estamos dando a la funci√≥n para que escoja la primera posici√≥n donde aparezca el texto que hayamos escogido.

-   As√≠, cuando vemos devuelto las respuestas `[1] 23653` y `[1] 44141` quiere decir que para `start_v` y `end_v` est√° asignado el primer valor donde aparece dichas expresiones , localizadas por primera vez en la posici√≥n 23653 y 44141 de nuestro texto.

#### PASO 2: Nuevo objeto

Creado nuestro punto de inicio y final de nuestra zona de trabajo, haremos un objeto que alberge dicho rango. Lo llamaremos: `navalny_fix`

```{r, eval=T}
navalny_fix <- stri_sub(navalny_raw, start_v, end_v)
length(navalny_fix)
```

Al iniciar el c√≥digo el valor que os ha tenido que recuperar, adem√°s de almacenar el objeto en la pesta√±a `Environment` de RStuido, es:

```         
[1] 1
```

#### ü§î¬øQu√© hemos hecho?

-   `navalny_fix` es el nombre del objeto que almacena la funci√≥n que ha sido asignada. En este caso, a trav√©s de `stri_sub`, estamos extrayendo una parte del texto `navalny_fix` . A diferencia del caso anterior, aqu√≠ le estamos pidiendo que, en vez de que recuper un n√∫mero detemrinado de caracteres, escoja todos los que hay comprendidos entre las posiciones que hemos dado a `start_v` y a `end_v`. Con esto nos aseguramos que el objeto `navalny_fix` siempre trabaje en los rangos que nos interesa analizar.

-   `length(navalny_fix)` es una expresi√≥n que usamos para comprobar cuandos valores existen en nuestro objeto. Es una forma de asegurarnos de que nuestro objeto solo tiene un vector y no es un conjunto de fragmentos de texto. Por eso, al introducirlo, el programa nos devuelve el valor 1 porque solo hay 1 valor dentro de nuestro objeto.

## üóëÔ∏è Limpiando datos

Ya tenemos nuestro set de datos, nos toca empezar a limpiar antes de tokenizar y empezar a analizar.

#### PASO 1: Comprobar la estructura del texto

Vamos a abrir un momento nuestro archivo para ver lo que contiene. Aqu√≠ te ense√±o las primeras l√≠neas:

```{r, echo= FALSE}
# Convertir el texto en un vector de l√≠neas
lines <- unlist(strsplit(navalny_fix, "\n"))

# Mostrar las primeras 20 l√≠neas del texto en formato raw
cat(lines[1:15], sep = "\n")

```

Aqu√≠ queda m√°s clara la estructura del texto:

-   Cada entrada del diario empieza con el a√±o en la primera l√≠nea

-   Dentro de cada a√±o, se dividen por d√≠as las entradas

-   Los p√°rrafos est√°n separados por saltos de l√≠nea

-   Las entradas est√°n separadas entre s√≠ tambi√©n por una linea en blanco.

A continuaci√≥n lo que vamos a hacer es crear un objeto lista[^3], en la que cada elemento ser√° una entrada del diario.

[^3]: Una **lista** es una estructura de datos que puede contener elementos de diferentes tipos (num√©rico, caract√©res, vectores o incluso otras listas) en un solo objeto. Cada elemento en una lista se puede acceder de forma individual usando √≠ndices. Esto se hace utilizando corchetes dobles. Por ejemplo si queremos ver el segundo elemento de la lista `lista`, lo indicaremos as√≠: `lista[[2]]`.

#### PASO 2: Conversi√≥n del texto en vectores

Ahora que comprendemos la estructura, vamos a separar el texto en entradas diarias, preservando la estructura de p√°rrafos dentro de cada entrada. Primero dividimos el texto en l√≠neas, donde cada l√≠nea se convierte en un elemento de un vector. Esto nos permite identificar las lineas que contienen fechas y separar las entradas.

```{r}
# Convertir el texto en un vector de l√≠neas
lines <- unlist(strsplit(navalny_fix, '\n')) # \n indica un salto de l√≠nea
```

#### ü§î¬øQu√© hemos hecho?

-   `strsplit()` divide el texto por saltos de l√≠nea (`"\n"`), creando un vector en el que cada l√≠nea es un elemento independiente.

-   Usamos `unlist()` para simplificar la estructura y trabajar con un vector plano.

```{r}
head(lines) # Veamos las primeras seis l√≠neas de nuestro objeto
```

#### PASO 3: Creaci√≥n de √≠ndices para identificar entradas

En este paso, vamos a crear los **√≠ndices** que nos permitir√°n identificar las l√≠neas en el texto que corresponden a cada a√±o y a cada d√≠a. Esto nos ayudar√° a estructurar las entradas en el pr√≥ximo paso.

1.  Utilizando expresiones regulares, vamos a identificar las l√≠neas que contengan s√≥lo el a√±o. Estas l√≠neas marcan el inicio de cada conjunto de entradas anuales

```{r}

year_indices <- grep("^\\d{4}$", lines)

print(year_indices) # Muestra las l√≠neas que contienen el a√±o
```

2.  Hemos identificado dos l√≠neas que incluyen el a√±o. Ahora haremos lo mismo para las l√≠neas que encuentren el mes y el d√≠a

```{r}

day_indices <- grep("^(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(st|nd|rd|th)?$", lines)

length(day_indices) # N√∫mero de entradas identificadas

```

#### ü§î¬øQu√© hemos hecho?

-   `year_indices` contiene los √≠ndices de las l√≠neas con los a√±os, es decir, las posiciones donde comienza cada a√±o en el texto. La expresi√≥n regular `^\\d{4}` busca cuatro d√≠gitos al inicio de la l√≠nea.

-   `month_day_indices` contiene los √≠ndices de las l√≠neas con fechas diarias, indicando el inicio de cada d√≠a dentro de los a√±os.

    -   `(January|February|...)`: Busca un mes escrito en ingl√©s.

    -   `\\s+`: Representa uno o m√°s espacios.

    -   `\\d{1,2}(st|nd|rd|th)?$`: Busca el d√≠a, que tendr√° uno o dos d√≠gitos y que puede ir seguido de "st", "nd", "rd", o "th" al final de la l√≠nea.

#### PASO 4: Creaci√≥n de entradas del diario

Ahora que tenemos los √≠ndices para los a√±os y d√≠as, podemos organizar el texto en **entradas anidadas**: cada a√±o ser√° un grupo principal, y dentro de cada a√±o, cada d√≠a ser√° una entrada individual.

El siguiente c√≥digo es un poco tocho, te intento explicar:

1.  Creamos las entradas por a√±o. Para esto utilizamos el objeto `year_indices` que construimos antes.
2.  Creamos sublistas dentro de cada a√±o con nuestro objeto `month_day_indices`.

```{r}
# Dividimos el texto en entradas anidadas (por a√±o y d√≠a)
yearly_entries <- lapply(
  seq_along(year_indices), 
  function(i) {
    start_year <- year_indices[i]
    end_year <- if (i < length(year_indices))
      year_indices[i + 1] - 1
    else
      length(lines)
    
    # Extraemos las l√≠neas correspondientes al a√±o actual
    year_lines <- lines[start_year:end_year]
    
    # Encontrar las entradas diarias dentro del a√±o actual
    day_indices <- grep(
      "^(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(st|nd|rd|th)?$",
      year_lines
      )
    
    # Crear una sublista para cada d√≠a dentro del a√±o
    entries <- lapply(seq_along(day_indices), function(j) {
      start_day <- day_indices[j]
      end_day <- if (j < length(day_indices))
        day_indices[j + 1] - 1
      else
        length(year_lines)
      year_lines[start_day:end_day]
      })
  
  # Devolver una lista con el a√±o y sus entradas diarias
  list(year = year_lines[1], entries = entries)
})

```

#### ü§î¬øQu√© hemos hecho?

-   Cada elemento en `yearly_entries` es un a√±o completo

    ```{r}
    print(yearly_entries[[1]]$year) # primer a√±o
    ```

-   Dentro de cada a√±o, `entries` contiene las entradas diarias como sublistas, donde cada d√≠a es un vector de p√°rrafos

    ```{r}
    length(yearly_entries[[1]]$entries)
    ```

Para ello, hemos combinado diferentes funciones en un s√≥lo script:

-   `lapply()` sirve para aplicar una funci√≥n a cada elemento de un vector o lista. En nuestro caso, itera sobre cada √≠ndice de `year_indices`, procesando el texto correspondiente a cada √±o. Genera una lista `yearly_indices`donde cada elemento representa un a√±o y sus entradas diarias. La segunda vez que la empleo es para crear sublistas para cada d√≠a dentro del a√±o, aplic√°ndola sobre `day_indices`.

-   `seq_along()` genera una secuencia de n√∫meros que corresponden a la longitud de un vector o lista. Lo utilizo para generar una secuencia de √≠ndices a iterar sobre los a√±os y sobre los d√≠as dentro de cada a√±o.

-   `grep()` busca patrones espec√≠ficos dentro de un vector, tal y como hicimos antes.

-   `if`dentro de `lapply()`define los l√≠mites de inicio y fin de cada a√±o.

-   `list()` crea la lista que almacena todas las entradas.

## üóø ¬°A la tokenizaci√≥n!

Realizada la limpieza de datos, es hora de preparar nuestro material para convertirlo en un objeto analizable. Para ello haremos tres cosas:

1.  Convertiremos todo el texto almacenado en `diary_v` en tokens.
2.  Pasaremos toda la informaci√≥n a min√∫sculas mediante la funci√≥n `tolower()`
3.  Eliminaremos los signos de puntuaci√≥n con la funci√≥n `remove_punct=TRUE`

#### PASO 1: tokenizaci√≥n

Para que el programa pueda analizar y realizar manipulaciones sobre las palabras de forma individualizada, vamos a convertir a cada una de ellas en peque√±os valores que llamamos **tokens.**

La funci√≥n `tokens()` del paquete `quanteda` es el que nos ayudar√° a realizar este trabajo.

```{r, eval=FALSE}
diary_v_toks <- tokens(diary_v)
```

#### **ü§î¬øQu√© significa esta expresi√≥n?**

Hemos creado un nuevo objeto: `diary_v_toks` donde los valores que tiene asignado han sido convertidos en una unidad manipulable llamado **token**. Esto nos permitir√° poder realizar los dos pasos siguientes.

#### PASO 2: eliminaci√≥n de las may√∫sculas.

R es sensible a las may√∫sculas y las toma como valor diferente a como ser√≠a en min√∫sculas. Por eso, para evitar errores, vamos a homogenizar el texto para que todo est√© en un solo formato: min√∫sculas.

Para ello crearemos un nuevo objeto: `diary_v_toks_lower` que albergar√° los valores del objeto del paso anterior, pero con la funci√≥n min√∫sculas aplicadas

```{r, eval=FALSE}
diary_v_toks_lower <- tokens_tolower(diary_v_toks)
```

#### PASO 3: eliminaci√≥n de los signos de puntuaci√≥n.

En nuestro texto, habr√° palabras que vengan unidas a un signo de puntuaci√≥n. Las comas, comillas y puntos pueden provocar que el programa identifica una misma palabra: `nalvany` y `nalvany.` como dos elementos distintos, lo que podr√≠a darnos un recuento distinto cuando hagamos b√∫squeda de patrones^\[4\]^.

Para realizar esta limpieza se har√° un nuevo objeto que llamaremos `nalvany_word_v`

```{r, eval=FALSE}
 nalvany_word_v <- tokens(diary_v_toks_lower, remove_punct = TRUE)
```

#### **ü§î¬øQu√© significa esta expresi√≥n?**

-   De nuevo el valor `nalvany_word_v` es un objeto al que se ha aplicado la funci√≥n `tokens`. Lo que le estamos diciendo a R es que el valor ya tokenizado `diary_v_toks_lower` vuelva a procesarlo tomando como par√°metro el valor `remove_punct=TRUE`

-   `remove_punct=TRUE` es el argumento que indica a `tokens()` que elimine toda la puntuaci√≥n del valor que le antecede, en este caso `diary_v_toks_lower`.

#### üîç Comprobaciones

Una vez hecho esta re-tokenizaci√≥n, vamos a crear un objeto que contenga la unidad total de valores que tiene el objeto `nalvany_word_v` . Esto nos ayudar√° a tener una idea total de la longitud del contenido, tambi√©n nos dar√° una idea de cu√°nto se ha reducido el texto al quitar caracteres y nos ahorrar√° tiempo para no tener que calcularlo cuando lo necesitemos

Aplicaremos la funci√≥n `ntoken` para averiguar la extensi√≥n total de nuestro texto.

```{r, eval=FALSE}
total_lenght <- ntoken(nalvany_word_v)
```

```         
text1 
 3697 #Nos tendr√≠a que devolver este valor
```

**¬øSab√≠as qu√©...?** Ô∏èü§ì‚òù

::: {.tip style="background-color: #A8E6A1; border-left: 5px solid #A8E6A1; padding: 10px"}
^~\[4\]~^ En ingl√©s, es posible que el texto venga acompa√±ado de ap√≥strofes como en los casos de `don't` y `he's`. Aqu√≠, `quanteda` no tomar√° las `'t` ni las `'s` como elementos aislados, sino que lo mantendr√° unida a la palabra para respetar el significado original.
:::

## ‚öôÔ∏è Las utilidades

Ahora que tenemos todo nuestro texto listo para trabajar, podemos realizar operaciones sencillas, tales como:

-   **Comprobar cuantas veces se repite una palabra y su localizaci√≥n.**

Esto es especialmente util para realizar an√°lisis de frecuencia o de sentimientos al conocer el contexto donde se menciona esa palabra.

```{r, eval=FALSE}
which(nalvany_word_v[["text1"]] == "putin") |>
+     head()
```

```         
[1] 1735 2771
```

-   **Localizar una palabra exacta a trav√©s de su n√∫mero de caracter.**

Como sabemos el n√∫mero total de caracteres que tiene gracias a la funci√≥n `ntoken` podemos recuperar cualquier elemento con solo introducir un valor concreto.

Esto nos podr√≠a ayudar para saber qu√© palabra se encuentra en una posici√≥n determinada para estudiar el contexto en el que aparece.

```{r, eval=FALSE}
nalvany_word_v[["text1"]][3666] 
```

```         
[1] "it"
```
