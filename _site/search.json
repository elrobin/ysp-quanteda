[
  {
    "objectID": "ysp_tutorial.html",
    "href": "ysp_tutorial.html",
    "title": "An√°lisis de textos",
    "section": "",
    "text": "El an√°lisis de textos es el proceso de convertir grandes vol√∫menes de texto en datos estructurados que nos permitan identificar patrones, tendencias, y relaciones dentro de ese texto. Es una t√©cnica fundamental en campos como las ciencias sociales, humanidades digitales, marketing, y pol√≠tica, ya que permite explorar y entender la informaci√≥n contenida en textos de manera cuantitativa.\nA medida que el volumen de informaci√≥n textual digitalizada ha crecido, la necesidad de analizar textos de forma sistem√°tica tambi√©n ha aumentado. Herramientas como quanteda permiten a los investigadores y analistas transformar textos en datos estructurados y cuantificables."
  },
  {
    "objectID": "ysp_tutorial.html#introducci√≥n-al-an√°lisis-cuantitativo-de-textos",
    "href": "ysp_tutorial.html#introducci√≥n-al-an√°lisis-cuantitativo-de-textos",
    "title": "An√°lisis de textos",
    "section": "",
    "text": "El an√°lisis de textos es el proceso de convertir grandes vol√∫menes de texto en datos estructurados que nos permitan identificar patrones, tendencias, y relaciones dentro de ese texto. Es una t√©cnica fundamental en campos como las ciencias sociales, humanidades digitales, marketing, y pol√≠tica, ya que permite explorar y entender la informaci√≥n contenida en textos de manera cuantitativa.\nA medida que el volumen de informaci√≥n textual digitalizada ha crecido, la necesidad de analizar textos de forma sistem√°tica tambi√©n ha aumentado. Herramientas como quanteda permiten a los investigadores y analistas transformar textos en datos estructurados y cuantificables."
  },
  {
    "objectID": "ysp_tutorial.html#para-qu√©-se-utiliza-el-an√°lisis-de-textos",
    "href": "ysp_tutorial.html#para-qu√©-se-utiliza-el-an√°lisis-de-textos",
    "title": "An√°lisis de textos",
    "section": "ü§î ¬øPara qu√© se utiliza el an√°lisis de textos?",
    "text": "ü§î ¬øPara qu√© se utiliza el an√°lisis de textos?\n\n\n\n\n\n\n\nAn√°lisis de sentimiento\nEvaluar el tono emocional de textos, como rese√±as o comentarios en redes sociales, para conocer las opiniones y actitudes de los usuarios.\n\n\nAn√°lisis de tendencias\nIdentificar la frecuencia de palabras o frases clave y su cambio a lo largo del tiempo.\n\n\nAn√°lisis del discurso\nExplorar c√≥mo ciertos temas o ideas son representados en la sociedad y su evoluci√≥n temporal\n\n\nClasificaci√≥n autom√°tica de textos\nOrganizar categ√≥ricamente textos y fragmentos de textos a trav√©s de t√©cnicas como topic modeling o similares"
  },
  {
    "objectID": "ysp_tutorial.html#quanteda-como-herramienta-para-el-an√°lisis-de-datos",
    "href": "ysp_tutorial.html#quanteda-como-herramienta-para-el-an√°lisis-de-datos",
    "title": "An√°lisis de textos",
    "section": "‚úçÔ∏è Quanteda como herramienta para el an√°lisis de datos",
    "text": "‚úçÔ∏è Quanteda como herramienta para el an√°lisis de datos\nQuanteda es un paquete de R dise√±ado espec√≠ficamente para el an√°lisis cuantitativo de textos. Su valor principal radica en que ofrece una forma r√°pida, flexible y eficiente de transformar textos en datos estructurados, permitiendo a los investigadores trabajar con grandes vol√∫menes de texto de manera sistem√°tica. Quanteda es ideal para el procesamiento inicial y an√°lisis exploratorio de textos."
  },
  {
    "objectID": "ysp_tutorial.html#por-qu√©-quanteda",
    "href": "ysp_tutorial.html#por-qu√©-quanteda",
    "title": "An√°lisis de textos",
    "section": "‚ÅâÔ∏è ¬øPor qu√© quanteda?",
    "text": "‚ÅâÔ∏è ¬øPor qu√© quanteda?\n\n\n\n\n\n\n\nFacilidad en la preprocesamiento\nPermite realizar tareas como tokenizaci√≥n y limpieza de datos de forma r√°pida y eficiente.\n\n\nAn√°lisis de frecuencias\nFacilita el c√°lculo de frecuencias de t√©rminos en un corpus, ideal para identificar palabras clave y patrones.\n\n\nExploraci√≥n de contexto (KWIC)\nLa funci√≥n KWIC permite analizar palabras en su contexto, proporcionando insights sobre el uso de ciertos t√©rminos.\n\n\nAn√°lisis de sentimiento y temas\nPermite aplicar diccionarios de sentimiento y realizar an√°lisis b√°sico de temas mediante co-ocurrencia de palabras.\n\n\nFlexibilidad y escalabilidad\nOptimizado para manejar grandes vol√∫menes de texto y se integra f√°cilmente con otros paquetes para an√°lisis avanzados."
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "An√°lisis y visualizaci√≥n de datos",
    "section": "",
    "text": "En este apartado mostramos algunos an√°lisis b√°sicos que se pueden hacer con quanteda. Vamos a ir in crescendo, de an√°lisis m√°s b√°sicos de frecuencias a t√©cnicas avanzadas de identificaci√≥n de patrones. Aqu√≠ te incluyo lo que tenemos preparado:"
  },
  {
    "objectID": "dataviz.html#an√°lisis-de-frecuencia-de-datos",
    "href": "dataviz.html#an√°lisis-de-frecuencia-de-datos",
    "title": "An√°lisis y visualizaci√≥n de datos",
    "section": "üî¢An√°lisis de frecuencia de datos",
    "text": "üî¢An√°lisis de frecuencia de datos\nEl an√°lisis de frecuencia de t√©rminos nos permite identificar las palabras m√°s comunes, facilitando un primer visionado de aquellos posibles temas recurrentes que pudieran ser interesantes de analizar.\n\n# Extraer el texto de cada entrada diaria y crear una lista plana \nunified_entries &lt;- unlist(lapply(yearly_entries, function(year) {   sapply(year$entries, function(entry) paste(entry, collapse = \" \")) }))    # Crear una matriz de frecuencias de t√©rminos a partir de los textos unificados  \ndfm_all &lt;- dfm(tokens(unified_entries))    \n# Mostrar las 10 palabras m√°s frecuentes  \ntop_words &lt;- topfeatures(dfm_all, n = 15)  \ntop_words\n\n   years     cell     like     dont      new   people      can      now \n      16       13       12       12       11       11       11       11 \n  russia     just everyone     miss      day    night   prison \n      11       10       10       10        9        9        9 \n\n\nü§î¬øQu√© hemos hecho?\n\nHemos unificado todo el texto en el objeto unified_entries de manera que ya no distinguimos por a√±o o fecha.\nHemos creado una matriz de frecuencias dfm_all para todo el conjunto de datos contando las veces que aparece cada palabra.\n\n\nDocument-feature matrix of: 6 documents, 1,040 features (88.17% sparse) and 0 docvars.\n           features\ndocs        january 12th two years behind bars truly original story one\n  entry_1_1       1    1   2     2      1    1     1        1     3   5\n  entry_1_2       1    0   3     3      0    0     0        0     0   0\n  entry_1_3       0    0   0     1      0    0     0        0     0   0\n  entry_1_4       0    0   0     0      0    0     0        0     0   0\n  entry_1_5       0    0   0     2      0    0     0        0     0   0\n  entry_1_6       0    0   0     1      0    0     0        0     0   0\n[ reached max_nfeat ... 1,030 more features ]\n\n\n\n{r, echo=FALSE} print(head(dfm_all))\nPara lograrlo, combinamos varias funciones:\n\nlapply(): Aplica una funci√≥n a cada elemento de yearly_entries, en este caso, iterando sobre cada a√±o en la lista. Dentro de esta funci√≥n, procesamos cada entrada diaria para obtener un texto unificado.\nsapply(): Anidado dentro de lapply(), sapply() recorre cada entrada diaria de un a√±o espec√≠fico (representada por year$entries). Nos permite aplicar paste() a cada entrada para unir todas las l√≠neas de la entrada diaria en un solo texto, sin generar sublistas adicionales.\nunlist(): Convierte el resultado de lapply() en un vector plano de textos diarios. Esto significa que todos los textos de yearly_entries se colocan en una sola estructura de lista sin anidaciones adicionales, facilitando el an√°lisis posterior.\ntokens(): Convierte cada entrada diaria unificada en tokens (es decir, en palabras individuales), lo cual es un paso necesario para aplicar dfm().\ndfm(): Crea una matriz de frecuencias de t√©rminos (document-feature matrix) a partir de los tokens generados en el paso anterior. En esta matriz, cada fila representa una entrada diaria y cada columna representa una palabra, con valores que indican la frecuencia de cada palabra en cada entrada.\ntopfeatures(dfm_all, n = 10): Extrae y muestra las 10 palabras m√°s frecuentes en todo el conjunto de datos.\n\nCombinando con el paquete de visualizaci√≥n ggplot2 podr√≠amos crear un gr√°fico de barras:\n\nlibrary(ggplot2)\n\n# Convertir el top 10 en un dataframe\ntop_words_df &lt;- data.frame(\n  word = names(top_words),\n  frequency = as.numeric(top_words)\n)\n\n# Crear el gr√°fico de barras\nggplot(top_words_df, aes(x = reorder(word, frequency), y = frequency)) +\n  geom_bar(stat = \"identity\", fill = \"black\") +\n  coord_flip() + \n  labs(title = \"Top 10 Most Frequent Words\",\n       x = \"Word\",\n       y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "dataviz.html#footnotes",
    "href": "dataviz.html#footnotes",
    "title": "An√°lisis y visualizaci√≥n de datos",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\n\n\n\n\n\nEl topic modeling (modelado de t√≥picos) es una t√©cnica de an√°lisis de texto que identifica temas latentes dentro de un conjunto de documentos. Agrupa palabras que tienden a aparecer juntas, permitiendo descubrir patrones tem√°ticos y estructuras ocultas en el texto.\n\n\n\n‚Ü©Ô∏é"
  },
  {
    "objectID": "dataprocessing.html",
    "href": "dataprocessing.html",
    "title": "Carga y procesamiento de textos",
    "section": "",
    "text": "Para seguir este tutorial por primera vez, deber√°s instalar una serie de paquetes1 que emplearemos: quanteda, readtext, dplyr y stringr. Esto se hace a trav√©s del comando: install.packages():\n\ninstall.packages(c(\"quanteda\", \"readtext\", \"dplyr\", \"stringi\"))\n\n\n\n\nquanteda\n\nPaquete de an√°lisis de textos, incluyendo tokenizaci√≥n, conteo y limpieza de textos\n\nreadtext\n\nPermite importar archivos de texto en varios formatos, facilitando la carga de datos\n\ndplyr\n\nHerramienta para manipulaci√≥n y transformaci√≥n de datos, √∫til para filtrar y organizar datos\n\nstringi\n\nConjunto de funciones para trabajar con texto, especialmente √∫til para limpieza y manejo de expresiones regulares.2\n\n\n¬øSab√≠as qu√©‚Ä¶? Ô∏èü§ì‚òù\n\nLa tokenizaci√≥n es el proceso de dividir un texto en unidades m√°s peque√±as llamdas tokens. Estas unidades pueden ser palabras, s√≠mbolos, frases o incluso caracteres dependiendo del tipo de an√°lisis que se vaya a realizar. En quanteda consideramos la palabra como la unidad m√≠nima de trabajo. Imag√≠nate que tienes la siguiente oraci√≥n:\n‚ÄúHola, ¬øc√≥mo est√°s?‚Äù\nLa tokenizaci√≥n de esta frase podr√≠a dar como resultado los siguientes tokens: ‚ÄúHola‚Äù, ‚Äú¬ø‚Äù, ‚Äúc√≥mo‚Äù, ‚Äúest√°s‚Äù, ‚Äú?‚Äù\nEsto es especialmente √∫til cuando estamos trabajando con estudios relacionados con frecuencias de palabras."
  },
  {
    "objectID": "dataprocessing.html#preparando-los-paquetes",
    "href": "dataprocessing.html#preparando-los-paquetes",
    "title": "Carga y procesamiento de textos",
    "section": "",
    "text": "Para seguir este tutorial por primera vez, deber√°s instalar una serie de paquetes1 que emplearemos: quanteda, readtext, dplyr y stringr. Esto se hace a trav√©s del comando: install.packages():\n\ninstall.packages(c(\"quanteda\", \"readtext\", \"dplyr\", \"stringi\"))\n\n\n\n\nquanteda\n\nPaquete de an√°lisis de textos, incluyendo tokenizaci√≥n, conteo y limpieza de textos\n\nreadtext\n\nPermite importar archivos de texto en varios formatos, facilitando la carga de datos\n\ndplyr\n\nHerramienta para manipulaci√≥n y transformaci√≥n de datos, √∫til para filtrar y organizar datos\n\nstringi\n\nConjunto de funciones para trabajar con texto, especialmente √∫til para limpieza y manejo de expresiones regulares.2\n\n\n¬øSab√≠as qu√©‚Ä¶? Ô∏èü§ì‚òù\n\nLa tokenizaci√≥n es el proceso de dividir un texto en unidades m√°s peque√±as llamdas tokens. Estas unidades pueden ser palabras, s√≠mbolos, frases o incluso caracteres dependiendo del tipo de an√°lisis que se vaya a realizar. En quanteda consideramos la palabra como la unidad m√≠nima de trabajo. Imag√≠nate que tienes la siguiente oraci√≥n:\n‚ÄúHola, ¬øc√≥mo est√°s?‚Äù\nLa tokenizaci√≥n de esta frase podr√≠a dar como resultado los siguientes tokens: ‚ÄúHola‚Äù, ‚Äú¬ø‚Äù, ‚Äúc√≥mo‚Äù, ‚Äúest√°s‚Äù, ‚Äú?‚Äù\nEsto es especialmente √∫til cuando estamos trabajando con estudios relacionados con frecuencias de palabras."
  },
  {
    "objectID": "dataprocessing.html#importaci√≥n-de-datos-de-texto",
    "href": "dataprocessing.html#importaci√≥n-de-datos-de-texto",
    "title": "Carga y procesamiento de textos",
    "section": "üìÑ‚û°Ô∏èüñ•Ô∏èImportaci√≥n de datos de texto",
    "text": "üìÑ‚û°Ô∏èüñ•Ô∏èImportaci√≥n de datos de texto\nVamos a vincular el archivo de¬†texto en la aplicaci√≥n de RStudios. Para ello vamos a cargar los paquetes que hemos instalado anteriormente¬†con el comando library(\"name\")\n\nlibrary(quanteda)\nlibrary(readtext)\nlibrary(dplyr)\nlibrary(stringi)\n\nEs importante resaltar que, si no llamamos antes el paquete, los comandos que introduzcamos despu√©s no funcionar√°n o nos dar√°n error. Aseg√∫rate de cargar siempre la librer√≠a antes de empezar a trabajar.\nUna vez cargados, el programa estar√° listo para leer nuestro archivo de texto. La formula que vamos a escribir para decirle a quanteda que archivo analizar ser√° el siguiente:\n\nnavalny_raw &lt;- as.character(readtext(\"NAVALNY.txt\"))\nnames(navalny_raw) &lt;- \"navalny\"\n\n‚ùóATENCI√ìN: Si por alguna raz√≥n hiciesemos alg√∫n cambio en el contenido del archivo, deberemos de aplicar el paso anterior de nuevo. Cuando cargamos un archivo en R, se guarda una copia y cualquier cambio en el original no se refleja autom√°ticamente.\nü§î¬øQu√© hemos hecho?\nEste comando carga el archivo NAVALNY.txt en el objeto navalny_raw, el cual contiene el contenido del texto. Vamos a desgranar este prompt para que pueda entenderse m√°s facil:\n\nnavalny_raw es un objeto en R que almacena el texto como una cadena de caracteres (character vector). En R hablamos de objetos para referirnos a los contenedores donde almacenamos datos e informaci√≥n. En el caso anterior, el objeto data_char_navalny almacena el texto plano que vamos a utilizar. Existen distintos objetos con diferentes datos almacenados: matrices, n√∫meros, listas jerarquizadas, as√≠ como un sin fin de combinaciones. A lo largo de este caso pr√°ctico trabajaremos con ellos para gestionar m√°s facilmente el an√°lisis cuantitativo.\n&lt;- emula a una flecha y b√°sicamente indica la direcci√≥n de la acci√≥n. Al objeto navalny_raw estamos aplic√°ndole una funci√≥n\nas.character(): Se trata de la funci√≥n que estamos aplicando. Esta funci√≥n convierte todo lo que se contiene dentro de ella en car√°cter. En R toda funci√≥n viene seguida por unos par√©ntesis dentro de los cu√°les se incluyen los par√°metros de dicha funci√≥n. Si no hay contenido, se aplican los par√°metros que la funci√≥n trae por defecto.\nreadtext: Aqu√≠ estamos empleando una funci√≥n expec√≠fica del paquete readtext. Si no hubi√©semos cargado el paquete anteriormente, podr√≠amos invocarlo espec√≠ficamente para activar esta funci√≥n con la notaci√≥n paquete::funci√≥n. Aqu√≠, lo que le estamos diciendo a RStudio es que, del paquete readtext, aplique espec√≠ficamente la funci√≥n lectura que casualmente tambi√©n se llama readtext . Dentro indicamos entrecomillada la ruta del archivo a importar. Un problema muy com√∫n que puede surgir a la hora de introducir la URL de la ubicaci√≥n del archivo es expresarlo con barras laterales izquierdas ‚Äù \\ ‚Äú, tal y como viene en la barra de direcci√≥n del explorador de archivos de Windows, en vez de la derecha‚Äù / ‚Äú. Si tienes problemas leer tu .txt ¬°prueba con hacer este cambio!\nnames(navalny_raw) &lt;- \"navalny\": Asigna un nombre al objeto que contiene el texto, facilitando su identificaci√≥n en futuros an√°lisis.\n\n\nüîç Comprobaciones\nEn el an√°lisis cuantitativo toda precauci√≥n es poca. Vamos a verificar que el texto ha sido le√≠do por el programa. Usaremos el paquete stringi para ver los primeros 75 caracteres de nuestro archivo y confirmar que los datos se cargaron bien.\n\n# Comprobar los primeros 75 caracteres del texto\nstri_sub(navalny_raw, 1, 75)\n\n[1] \"2022\\nJanuary 17th\\nExactly one year ago today I came home, to Russia.\\nI didn\"\n\n\nSi hemos hecho los pasos bien, tendr√©is que haber recibir este texto de vuelta:\n[1] \"2022\\nJanuary 17th\\nExactly one year ago today I came home, to Russia.\\nI didn\""
  },
  {
    "objectID": "dataprocessing.html#acotando-el-texto-a-analizar",
    "href": "dataprocessing.html#acotando-el-texto-a-analizar",
    "title": "Carga y procesamiento de textos",
    "section": "üóÉÔ∏èAcotando el texto a analizar",
    "text": "üóÉÔ∏èAcotando el texto a analizar\nNuestro siguiente objetivo es seleccionar qu√© partes del texto vamos aplicar el an√°lisis cuantitativo. Puede que nuestro foco de inter√©s sea algun apartado concreto de nuestro material, por lo que vamos a crear un objeto que albergue un rango determinado dentro de nuestro fichero txt . Con esto, nos quitaremos toda la informaci√≥n innecesaria que puede ensuciar nuestros resultados.\nEl proceso que vamos a realizar a continuaci√≥n es muy √∫til cuando los archivos que manejamos tienen ligados metadatos. Normalmente, esta metadata suele ser m√°s un dolor de cabeza que otra cosa y es recomendable realizar una limpieza previa para que esos datos no se mezclen con el contenido de nuestro an√°lisis. En este apartado seguiremos trabajando con el paquete stringi.\nSi el texto contiene secciones que no necesitamos para el an√°lisis, podemos filtrarlas o limpiarlas en esta etapa.\n\nPASO 1: Identificaci√≥n de comienzo y fin del texto\nPara crear el objeto que albergue el rango de texto a analizar deberemos empezar indicando donde empieza y termina nuestra selecci√≥n. Para ello, crearemos dos valores de posici√≥n: start_v y end_v, donde start_ ser√°: ‚Äú2023, January 12th‚Äù y end_v ‚ÄúAlexei Navalny died‚Äù.\nLocalizado el rango que queremos, la forma de expresarlo en el programa ser√≠a el siguiente:\n\n(start_v &lt;- stri_locate_first_fixed(navalny_raw, \"2023\\nJanuary 12th\")[1])\n\n[1] 23654\n\n\n\n(end_v &lt;- stri_locate_last_fixed(navalny_raw, \"Alexei Navalny died\")[1])\n\n[1] 44099\n\n\nSi lo hemos aplicado bien, la funci√≥n deber√≠a de devolver los siguientes resultados\n\nPara start_value\n[1] 23654\n\n\n\nPara end_value\n[1] 44099\n\nü§î¬øQu√© hemos hecho?\n\nTanto start_v como end_v son nombres que hemos asignado a la posici√≥n espec√≠ficas del texto. En s√≠, no significan nada. Solo decimos, a trav√©s de ‚Äú&lt;-‚Äù que dichos nombres albergan una funci√≥n de posicionamiento.\nLas funciones del paquete stringi: stri_locate_first_fixed y stri_locate_last_fixed buscan y encuentran la primera coincidencia del valor entrecomillado que precede a nuestro objeto data_char_NALVANY\nEl [1] es un indicador que le estamos dando a la funci√≥n para que escoja la primera posici√≥n donde aparezca el texto que hayamos escogido.\nAs√≠, cuando vemos devuelto las respuestas [1] 23654 y [1] 44099 quiere decir que para start_v y end_v est√° asignado el primer valor donde aparece dichas expresiones , localizadas por primera vez en la posici√≥n 23654 y 44099 de nuestro texto.\n\n\n\nPASO 2: Nuevo objeto\nCreado nuestro punto de inicio y final de nuestra zona de trabajo, haremos un objeto que alberge dicho rango. Lo llamaremos: navalny_fix\n\nnavalny_fix &lt;- stri_sub(navalny_raw, start_v, end_v)\nlength(navalny_fix)\n\n[1] 1\n\n\nAl iniciar el c√≥digo el valor que os ha tenido que recuperar, adem√°s de almacenar el objeto en la pesta√±a Environment de RStudio, es:\n[1] 1\nü§î¬øQu√© hemos hecho?\n\nnavalny_fix es el nombre del objeto que almacena la funci√≥n que ha sido asignada. En este caso, a trav√©s de stri_sub, estamos extrayendo una parte del texto navalny_fix . A diferencia del caso anterior, aqu√≠ le estamos pidiendo que, en vez de que recuper un n√∫mero detemrinado de caracteres, escoja todos los que hay comprendidos entre las posiciones que hemos dado a start_v y a end_v. Con esto nos aseguramos que el objeto navalny_fix siempre trabaje en los rangos que nos interesa analizar.\nlength(navalny_fix) es una expresi√≥n que usamos para comprobar cuandos valores existen en nuestro objeto. Es una forma de asegurarnos de que nuestro objeto solo tiene un vector y no es un conjunto de fragmentos de texto. Por eso, al introducirlo, el programa nos devuelve el valor 1 porque solo hay 1 valor dentro de nuestro objeto."
  },
  {
    "objectID": "dataprocessing.html#limpiando-datos",
    "href": "dataprocessing.html#limpiando-datos",
    "title": "Carga y procesamiento de textos",
    "section": "üóëÔ∏è Limpiando datos",
    "text": "üóëÔ∏è Limpiando datos\nYa tenemos nuestro set de datos, nos toca empezar a limpiar antes de tokenizar y empezar a analizar.\n\nPASO 1: Comprobar la estructura del texto\nVamos a abrir un momento nuestro archivo para ver lo que contiene. Aqu√≠ te ense√±o las primeras l√≠neas:\n\n\n2023\nJanuary 12th\nIn my two years behind bars, my only truly original story is the one about the\npsycho. Everything else has been told and described numerous times. If you\nopen any book by a Soviet dissident, there will be endless stories of punishment\ncells, hunger strikes, violence, provocations, lack of medical care. Nothing new.\nBut my story about the psycho is fresh; at least,I‚Äôve never seen or heard\nanything like it\n\nSo, let me give you an idea about the shizo, the place where I sit all the time. It\nis a narrow corridor with cells on either side. The metal doors offer little to no\nsoundproofing, plus there are ventilation holes above the doors, so two people\nsitting in opposite cells can have a conversation without even raising their\nvoices. This is the main reason there has never been anyone in the cell opposite\nmine, or in my entire eight-cell section. I am the only one there, and I have\n\n\nAqu√≠ queda m√°s clara la estructura del texto:\n\nCada entrada del diario empieza con el a√±o en la primera l√≠nea\nDentro de cada a√±o, se dividen por d√≠as las entradas\nLos p√°rrafos est√°n separados por saltos de l√≠nea\nLas entradas est√°n separadas entre s√≠ tambi√©n por una linea en blanco.\n\nA continuaci√≥n lo que vamos a hacer es crear un objeto lista3, en la que cada elemento ser√° una entrada del diario.\n\n\nPASO 2: Conversi√≥n del texto en vectores\nAhora que comprendemos la estructura, vamos a separar el texto en entradas diarias, preservando la estructura de p√°rrafos dentro de cada entrada. Primero dividimos el texto en l√≠neas, donde cada l√≠nea se convierte en un elemento de un vector. Esto nos permite identificar las lineas que contienen fechas y separar las entradas.\n\n# Convertir el texto en un vector de l√≠neas\nlines &lt;- unlist(strsplit(navalny_fix, '\\n')) # \\n indica un salto de l√≠nea\n\nü§î¬øQu√© hemos hecho?\n\nstrsplit() divide el texto por saltos de l√≠nea (\"\\n\"), creando un vector en el que cada l√≠nea es un elemento independiente.\nUsamos unlist() para simplificar la estructura y trabajar con un vector plano.\n\n\nhead(lines) # Veamos las primeras seis l√≠neas de nuestro objeto\n\n[1] \"2023\"                                                                             \n[2] \"January 12th\"                                                                     \n[3] \"In my two years behind bars, my only truly original story is the one about the\"   \n[4] \"psycho. Everything else has been told and described numerous times. If you\"       \n[5] \"open any book by a Soviet dissident, there will be endless stories of punishment\" \n[6] \"cells, hunger strikes, violence, provocations, lack of medical care. Nothing new.\"\n\n\n\n\nPASO 3: Creaci√≥n de √≠ndices para identificar entradas\nEn este paso, vamos a crear los √≠ndices que nos permitir√°n identificar las l√≠neas en el texto que corresponden a cada a√±o y a cada d√≠a. Esto nos ayudar√° a estructurar las entradas en el pr√≥ximo paso.\n\nUtilizando expresiones regulares, vamos a identificar las l√≠neas que contengan s√≥lo el a√±o. Estas l√≠neas marcan el inicio de cada conjunto de entradas anuales\n\n\nyear_indices &lt;- grep(\"^\\\\d{4}$\", lines)\n\nprint(year_indices) # Muestra las l√≠neas que contienen el a√±o\n\n[1]   1 295\n\n\n\nHemos identificado dos l√≠neas que incluyen el a√±o. Ahora haremos lo mismo para las l√≠neas que encuentren el mes y el d√≠a\n\n\nday_indices &lt;- grep(\"^(January|February|March|April|May|June|July|August|September|October|November|December)\\\\s+\\\\d{1,2}(st|nd|rd|th)?$\", lines)\n\nlength(day_indices) # N√∫mero de entradas identificadas\n\n[1] 11\n\n\nü§î¬øQu√© hemos hecho?\n\nyear_indices contiene los √≠ndices de las l√≠neas con los a√±os, es decir, las posiciones donde comienza cada a√±o en el texto. La expresi√≥n regular ^\\\\d{4} busca cuatro d√≠gitos al inicio de la l√≠nea.\nmonth_day_indices contiene los √≠ndices de las l√≠neas con fechas diarias, indicando el inicio de cada d√≠a dentro de los a√±os.\n\n(January|February|...): Busca un mes escrito en ingl√©s.\n\\\\s+: Representa uno o m√°s espacios.\n\\\\d{1,2}(st|nd|rd|th)?$: Busca el d√≠a, que tendr√° uno o dos d√≠gitos y que puede ir seguido de ‚Äúst‚Äù, ‚Äúnd‚Äù, ‚Äúrd‚Äù, o ‚Äúth‚Äù al final de la l√≠nea.\n\n\n\n\nPASO 4: Creaci√≥n de entradas del diario\nAhora que tenemos los √≠ndices para los a√±os y d√≠as, podemos organizar el texto en entradas anidadas: cada a√±o ser√° un grupo principal, y dentro de cada a√±o, cada d√≠a ser√° una entrada individual.\nEl siguiente c√≥digo es un poco tocho, te intento explicar:\n\nCreamos las entradas por a√±o. Para esto utilizamos el objeto year_indices que construimos antes.\nCreamos sublistas dentro de cada a√±o con nuestro objeto month_day_indices.\n\n\n# Dividimos el texto en entradas anidadas (por a√±o y d√≠a)\nyearly_entries &lt;- lapply(\n  seq_along(year_indices), \n  function(i) {\n    start_year &lt;- year_indices[i]\n    end_year &lt;- if (i &lt; length(year_indices))\n      year_indices[i + 1] - 1\n    else\n      length(lines)\n    \n    # Extraemos las l√≠neas correspondientes al a√±o actual\n    year_lines &lt;- lines[start_year:end_year]\n    \n    # Encontrar las entradas diarias dentro del a√±o actual\n    day_indices &lt;- grep(\n      \"^(January|February|March|April|May|June|July|August|September|October|November|December)\\\\s+\\\\d{1,2}(st|nd|rd|th)?$\",\n      year_lines\n      )\n    \n    # Crear una sublista para cada d√≠a dentro del a√±o\n    entries &lt;- sapply(seq_along(day_indices), function(j) {\n      start_day &lt;- day_indices[j]\n      end_day &lt;- if (j &lt; length(day_indices))\n        day_indices[j + 1] - 1\n      else\n        length(year_lines)\n      \n      # Unir todas las l√≠neas de la entrada diaria en un solo texto\n      paste(year_lines[start_day:end_day], collapse = \" \")\n      }, simplify = TRUE)\n  \n  # Devolver una lista con el a√±o y sus entradas diarias\n  list(year = year_lines[1], entries = entries)\n})\n\nü§î¬øQu√© hemos hecho?\n\nCada elemento en yearly_entries es un a√±o completo\n\nprint(yearly_entries[[1]]$year) # primer a√±o\n\n[1] \"2023\"\n\n\nDentro de cada a√±o, entries contiene las entradas diarias como sublistas, donde cada d√≠a es un vector de p√°rrafos\n\nlength(yearly_entries[[1]]$entries)\n\n[1] 9\n\n\n\nPara ello, hemos combinado diferentes funciones en un s√≥lo script:\n\nlapply() sirve para aplicar una funci√≥n a cada elemento de un vector o lista. En nuestro caso, itera sobre cada √≠ndice de year_indices, procesando el texto correspondiente a cada √±o. Genera una lista yearly_indicesdonde cada elemento representa un a√±o y sus entradas diarias. La segunda vez que la empleo es para crear sublistas para cada d√≠a dentro del a√±o, aplic√°ndola sobre day_indices.\nseq_along() genera una secuencia de n√∫meros que corresponden a la longitud de un vector o lista. Lo utilizo para generar una secuencia de √≠ndices a iterar sobre los a√±os y sobre los d√≠as dentro de cada a√±o.\ngrep() busca patrones espec√≠ficos dentro de un vector, tal y como hicimos antes.\npaste() para unir todas las l√≠neas del texto en una sola por entrada y separarlas por un espacio.\nifdentro de sapply()define los l√≠mites de inicio y fin de cada a√±o y con el par√°metro simplify = TRUE nos devolver√° un vector de cadenas de texto por entrada.\nlist() crea la lista que almacena todas las entradas.\n\n\n\nPASO 5: Asignaci√≥n de nombres a entradas\nDe cara al an√°lisis de datos global, deberemos nombrar cada una de las entradas para que luego seamos capaces de identificarlas.\n\n# Asignar nombres √∫nicos a cada entrada en yearly_entries\nyearly_entries &lt;- lapply(seq_along(yearly_entries), function(i) {\n  year &lt;- yearly_entries[[i]]\n  \n  # Asignar nombres √∫nicos a cada entrada diaria\n  names(year$entries) &lt;- paste0(\"entry_\", i, \"_\", seq_along(year$entries))\n  \n  year  # Devolver el a√±o con las entradas renombradas\n})\n\nü§î¬øQu√© hemos hecho?\n\nHemos aplicado la funci√≥n names()a cada entrada del diario dentro de cada a√±o. Esta funci√≥n asigna un nombre a cada elemento dentro de un objeto. En una tabla de Excel ser√≠a lo equivalente a ponerle un nombre a cada columna de nuestro set de datos.\nPara asignar los nombres de manera autom√°tica y que estos sean √∫nicos empleamos la funci√≥n paste0() que concatena diferentes secuencias de texto, en nuestro caso concatenamos cuatro elementos:\n\nUn texto: \"entry_\".\nEl √≠ndice del a√±o i.\nEl √≠ndice de la entrada, en este caso lo determinamos con la funci√≥n seq_along() que va contando la posici√≥n de cada uno de los elementos entries dentro de la lista year.\n\n\n\n\n[1] \"entry_1_1\" \"entry_1_2\" \"entry_1_3\" \"entry_1_4\" \"entry_1_5\" \"entry_1_6\"\n[7] \"entry_1_7\" \"entry_1_8\" \"entry_1_9\""
  },
  {
    "objectID": "dataprocessing.html#a-la-tokenizaci√≥n",
    "href": "dataprocessing.html#a-la-tokenizaci√≥n",
    "title": "Carga y procesamiento de textos",
    "section": "üóø ¬°A la tokenizaci√≥n!",
    "text": "üóø ¬°A la tokenizaci√≥n!\nYa tenemos nuestro texto bien organizado y estructurado. Toca dividir a√∫n m√°s y limpiar. Para ello vamos a hacer lo siguiente:\n\nConvertimos el texto en min√∫sculas y eliminaos los signos de puntuaci√≥n\nTokenizamos el texto dividi√©ndolo en palabras. De manera que nuestra unidad de an√°lisis ser√° la palabra4.\nEliminamos todas las palabras vac√≠as5 para quedarnos s√≥lo con aquellas relevantes para nuestro an√°lisis.\n\n\nPASO 1: Texto en min√∫scula y puntuaci√≥n fuera\nPara asegurarnos que palabras id√©nticas no se traten como diferentes por su formato, converitmos todo el texto a min√∫sculas y eliminamos puntuaci√≥n.\n\n# Convertir cada entrada diaria a min√∫sculas y eliminar signos de puntuaci√≥n\nyearly_entries &lt;- lapply(yearly_entries, function(year) {\n  year$entries &lt;- lapply(year$entries, function(entry) {\n    # Convertir el texto a min√∫sculas y eliminar puntuaci√≥n\n    entry &lt;- char_tolower(entry)\n    entry &lt;- gsub(\"[[:punct:]]\", \"\", entry)\n    entry\n  })\n  year  # Devolver la lista de a√±o modificada\n})\n\nü§î¬øQu√© hemos hecho?\n\nLa funci√≥n char_tolower() convierte el texto en min√∫sculas.\nLa funci√≥n gsub() sustituye un patr√≥n de texto por otro. En nuestro caso, le hemos pedido que busque cualquier signo de puntuaci√≥n empleando la expresi√≥n regular [[:punct:]] y la reemplace por nada.\nDespu√©s hemos pedido que incluya estos cambios nuevamente en nuestro objeto yearly_entries.\n\nSi observas las primeras l√≠neas de una entrada, ver√°s que todo ha funcionado tal y como esper√°bamos:\n\n\n[1] \"january 12th in my two years behind bars my only\"\n\n\n\n\nPASO 2: Tokenizaci√≥n\nPara que el programa pueda analizar y realizar manipulaciones sobre las palabras de forma individualizada, vamos a convertir a cada una de ellas en peque√±os valores que llamamos tokens.\n\n# Tokenizar cada entrada diaria dentro de cada a√±o\nyearly_entries &lt;- lapply(yearly_entries, function(year) {\n  year$entries &lt;- lapply(year$entries, function(entry) {\n    tokens(entry, what = \"word\")\n  })\n  year  # Devolver la lista de a√±o modificada\n})\n\nü§î¬øQu√© hemos hecho?\n\nSiguiendo la misma estructura de la vez anterior, hemos incorporado la funci√≥n tokens() y lo hemos aplicado al objeto entry\nAdem√°s, hemos utilizado el par√°metro what= en el que indicamos el nivel. En nuestro caso tokenizamos por palabras. Otras opciones son por caracteres (character) y frases (sentence).\n\nAhora en lugar de contener un listado de filas por entrada, lo que tengo es una bolsa de palabras:\n\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"january\"  \"12th\"     \"in\"       \"my\"       \"two\"      \"years\"   \n [7] \"behind\"   \"bars\"     \"my\"       \"only\"     \"truly\"    \"original\"\n[ ... and 750 more ]\n\n\n\n\nPASO 3: Eliminaci√≥n de palabras vac√≠as\nPara centrarnos en las palabras significativas, eliminamos las palabras vac√≠as (stopwords), que suelen ser t√©rminos comunes y poco informativos, como ‚Äúel‚Äù, ‚Äúde‚Äù, ‚Äúy‚Äù. Esto permite que el an√°lisis se centre en t√©rminos con m√°s contenido sem√°ntico.\n\n# Eliminar palabras vac√≠as en ingl√©s en cada entrada diaria\nyearly_entries &lt;- lapply(yearly_entries, function(year) {\n  year$entries &lt;- lapply(year$entries, function(entry) {\n    tokens_remove(entry, pattern = stopwords(\"en\")) \n  })\n  year  # Devolver la lista de a√±o modificada\n})\n\nü§î¬øQu√© hemos hecho?\n\nAqu√≠ empleamos la funci√≥n tokens_remove() otra vez aplicada al objeto entry, en este caso empleamos el par√°metro pattern = para indicar que eliminaremos los tokens que representen palabras vac√≠as, en par√©ntesis incluimos la lengua a trav√©s de su c√≥digo ISO, en nuestro caso el ingl√©s. Aqu√≠ tienes el listado completo de idiomas. El paquete stopwords permite asimismo crear y/o a√±adir tus propias palabras vac√≠as.\n\nF√≠jate c√≥mo, en comparaci√≥n con el fragmento anterior, se han eliminado palabras como ‚Äúin‚Äù, ‚Äúmy‚Äù u ‚Äúonly‚Äù.\n\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"january\"    \"12th\"       \"two\"        \"years\"      \"behind\"    \n [6] \"bars\"       \"truly\"      \"original\"   \"story\"      \"one\"       \n[11] \"psycho\"     \"everything\"\n[ ... and 371 more ]\n\n\n\n\n\n\n\n\nComo habr√°s notado, es posible juntar todos estos pasos en uno s√≥lo. Aqu√≠ lo mostramos por trozos para que vayas comprendiendo el proceso, pero podr√≠amos hacer todo esto de una vez.\n\n\n\n¬øSab√≠as qu√©‚Ä¶? Ô∏èü§ì‚òù\n\nEn ingl√©s, es posible que el texto venga acompa√±ado de ap√≥strofes como en los casos de don't y he's. Aqu√≠, quanteda no tomar√° las 't ni las 's como elementos aislados, sino que lo mantendr√° unida a la palabra para respetar el significado original."
  },
  {
    "objectID": "dataprocessing.html#footnotes",
    "href": "dataprocessing.html#footnotes",
    "title": "Carga y procesamiento de textos",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\n\n\n\n\n\nR es un lenguaje de programaci√≥n abierto y colaborativo que sigue una estructura totalmente descentralizada. Cuando instalamos R por primera vez, s√≥lo instalamos sus funcionalidades b√°sicas. Todas aquellas funcionalidades adicionales llevadas a cabo por terceras personas deben instalarse en lo que se denominan paquetes o libraries en ingl√©s.\nCada vez que vayas a emplear un paquete, debes cargarlo, por defecto, cada vez que abres R estos paquetes no est√°n cargados.\n\n\n\n‚Ü©Ô∏é\n\n\n\n\n\n\nUna expresi√≥n regular es un patr√≥n de b√∫squeda utilizado para manipular texto espec√≠fico en una cadena. Facilita tareas como eliminar caracteres no deseados o extraer informaci√≥n espec√≠fica (e.g., fechas o n√∫meros). Es especialmente √∫til en la limpieza y preprocesamiento de datos textuales.\n\n\n\n‚Ü©Ô∏é\n\n\n\n\n\n\nUna lista es una estructura de datos qeu puede contener elementos de diferentes tipos (num√©rico, caract√©res, vectores o incluso otras listas) en un solo objecto. Cada elemento en una lista se puede acceder de forma individual usando √≠ndices. Esto se hace utilizando corchetes dobles. Por ejemplo si queremos ver el segundo elemento de la lista lista, lo indicaremos as√≠: lista[[2]].\n\n\n\n‚Ü©Ô∏é\n\n\n\n\n\n\nAqu√≠ es importante diferenciar entre an√°lisis textual y un an√°lisis sem√°ntico. En el an√°lisis de textos examinamos cuestiones como la frecuencia de las palabras, patrones o estructura, sin considerar el significado de cada palabra. Se tratar√≠a de un paso previo al an√°lisis sem√°ntico donde nos centramos en el significado y el contexto de las palabras.\n\n\n\n‚Ü©Ô∏é\n\n\n\n\n\n\nLas palabras vac√≠as son t√©rminos comunes (como ‚Äúel‚Äù, ‚Äúde‚Äù, ‚Äúy‚Äù) que suelen aparecer con mucha frecuencia en el texto, pero aportan poco significado o valor informativo al an√°lisis. Estas palabras se eliminan generalmente para centrar el an√°lisis en los t√©rminos m√°s relevantes.\n\n\n\n‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bienvenidos",
    "section": "",
    "text": "Datos b√°sicos del curso\n\n\n\nMateriales preparados para el curso impartido en la plataforma YoSigo UGR de la Universidad de Granada por Sergio Castro-Cortacero y Nicol√°s Robinson-Garc√≠a el d√≠a 8 de noviembre de 2024."
  },
  {
    "objectID": "index.html#objetivos-de-la-sesi√≥n",
    "href": "index.html#objetivos-de-la-sesi√≥n",
    "title": "Bienvenidos",
    "section": "üéØObjetivos de la sesi√≥n",
    "text": "üéØObjetivos de la sesi√≥n\nEsta web contiene los contenidos del curso de YoSigo UGR Introducci√≥n al an√°lisis de textos con Quanteda. En esta sesi√≥n queremos iniciarte en el fant√°stico mundo del an√°lisis de textos. Para ello vamos a utilizar el lenguaje de programaci√≥n R y el paquete de an√°lisis de textos Quanteda. Para ello vamos a explorar algunas cuestiones b√°sicas para el an√°lisis de textos como es la creaci√≥n de un corpus, el proceso de tokenizaci√≥n y algunos an√°lisis b√°sicos. Al terminar esta sesi√≥n deber√≠as poder hacer lo siguiente:\n\nEntender los fundamentos b√°sicos de miner√≠a de datos.\nSer capaz de importar y tokenizar un texto empleando el paquete de R quanteda.\nRealizar un an√°lisis pr√°ctico utilizando un corpus de texto real."
  },
  {
    "objectID": "index.html#antes-de-comenzar",
    "href": "index.html#antes-de-comenzar",
    "title": "Bienvenidos",
    "section": "ü´∂Antes de comenzar",
    "text": "ü´∂Antes de comenzar\nPara poder repetir las cosas que vamos a hacer en este curso y poder realizar tus propios an√°lisis, deber√°s de tener instalado en tu ordenador los siguientes programas (inst√°lalos en el orden que te indico):\n\nR. Este es el lenguaje programaci√≥n que emplearemos. Se trata de un lenguaje abierto y gratuito desarrollado espec√≠ficamente para el an√°lisis estad√≠stico. Aunque como con cualquier otro lenguaje de programaci√≥n, es muy flexible y puede emplearse para diferentes funciones. Cuenta con una amplia comunidad de usuarios que crean y mantienen sus funcionalidades creando lo que se conoce como paquetes o librer√≠as.\nRStudio. Se trata de una interfaz de R que facilita algunas operaciones. Aunque en principio, R puede emplearse sin interfaz, RStudio incluye los principales paquetes adicionales as√≠ como algunas funcionalidades que pueden ser muy √∫tiles para mantenernos cuerdos al adentrarnos en el mundo de la programaci√≥n.\n\n\nüí° Aqu√≠ te daremos el c√≥digo muy mascadito, pero si no tienes experiencia programando con R y deseas conocer las bases de su gram√°tica, te recomiendo los siguientes recursos:\n\nEl curso de yosigo Introducci√≥n pr√°ctica a la ciencia de datos en R ‚Äì REDUX by Wenceslao Arroyo-Machado\nEl libro online R for Data Science by Hadley Wickham, uno de los m√°ximos exponentes en R"
  },
  {
    "objectID": "dataviz.html#nube-de-palabras",
    "href": "dataviz.html#nube-de-palabras",
    "title": "An√°lisis y visualizaci√≥n de datos",
    "section": "üå•Ô∏è Nube de palabras",
    "text": "üå•Ô∏è Nube de palabras\nUna nube de palabras permite identificar visualmente las palabras m√°s frecuentes en el texto. Las palabras m√°s grandes en la nube son las que aparecen con mayor frecuencia, lo cual facilita una comprensi√≥n r√°pida de los t√©rminos m√°s relevantes en el contenido.\nUsaremos la matriz de frecuencias de t√©rminos dfm_all que creamos previamente para generar la nube de palabras.\n\nlibrary(quanteda.textplots)\n\n# Generar la nube de palabras a partir de la matriz de frecuencias\ntextplot_wordcloud(dfm_all, max_words = 100, color = \"orange\")\n\n\n\n\n\n\n\n\nü§î¬øQu√© hemos hecho?\n\ntextplot_wordcloud() genera una nube de palabras a partir de la matriz de frecuencias dfm_all.\n\nmax_words = 100 limita la visualizaci√≥n a las 100 palabras m√°s frecuentes, para que la nube sea m√°s clara y enfocada en los t√©rminos clave.\ncolor = asigna el color de las palabras para una visualizaci√≥n consistente."
  },
  {
    "objectID": "dataviz.html#an√°lisis-de-dispersi√≥n-l√©xica",
    "href": "dataviz.html#an√°lisis-de-dispersi√≥n-l√©xica",
    "title": "An√°lisis y visualizaci√≥n de datos",
    "section": "üìçAn√°lisis de dispersi√≥n l√©xica",
    "text": "üìçAn√°lisis de dispersi√≥n l√©xica\nLa dispersi√≥n l√©xica nos permite ver en qu√© partes del texto aparecen ciertas palabras clave. Esto puede ser particularmente √∫til si queremos rastrear c√≥mo cambia el uso de ciertos t√©rminos importantes a lo largo del tiempo o en diferentes contextos.\nUsaremos la funci√≥n textplot_xray() de quanteda.textplots para generar un gr√°fico de dispersi√≥n de las palabras clave en el conjunto de datos. Para ello debermos instalar primero este paquete.\n\n# Librer√≠a necesaria\nlibrary(quanteda.textplots)\n\n# Palabras clave para el an√°lisis de dispersi√≥n l√©xica\nkeywords &lt;- c(\"cell\", \"russia\", \"people\", \"shizo\")\n\n# Unificar todas las entradas en un solo texto\nfull_text &lt;- paste(unified_entries, collapse = \" \")\n\n# Crear tokens a partir del texto completo\ntokens_full_text &lt;- tokens(full_text)\n\n# Generar gr√°fico de dispersi√≥n l√©xica para todo el texto\ntextplot_xray(kwic(tokens_full_text, pattern = keywords))\n\n\n\n\n\n\n\n\nü§î¬øQu√© hemos hecho?\n\nHemos seleccionado los t√©rminso a analizar bas√°ndonos en el an√°lisis de frecuencias anterior\nCon la funci√≥n paste(..., collapse = \" \") hemos unido todas las entradas en el objeto full_text, formando un solo texto continuo sin distinguir por a√±o o entrada.\nHemos creado los tokens a partir del texto unificado con la funci√≥n tokens() para poder trabajar a nivel de palabras\nLa funci√≥n kwick() nos sirve para extraer las posiciones de las palabras clave del texto completo.\ntextplot_xray() viene del paquete quanteda.textplots y nos permite visualizar la dispersi√≥n l√©xica."
  },
  {
    "objectID": "dataviz.html#an√°lisis-de-concordancias-kwic",
    "href": "dataviz.html#an√°lisis-de-concordancias-kwic",
    "title": "An√°lisis y visualizaci√≥n de datos",
    "section": "üîç An√°lisis de concordancias (KWIC)",
    "text": "üîç An√°lisis de concordancias (KWIC)\nEste an√°lisis muestra cada aparici√≥n de las palabras clave junto con el contexto inmediato, lo que permite estudiar c√≥mo se usan ciertos t√©rminos en diferentes partes del texto.\nEn este ejemplo vamos a analizar las palabras que preceden y anteceden al t√©rmino shizo.\n\n# Extraer concordancias de las palabras clave\nconcordancias &lt;- kwic(tokens_full_text, pattern = \"shizo\", window = 2)\nprint(concordancias)\n\nKeyword-in-context with 7 matches.                                                                \n   [text1, 47]        give idea | shizo | place sit             \n  [text1, 498]          now add | shizo | list places           \n  [text1, 532]    requires sent | shizo | must examined         \n  [text1, 554]         weve put | shizo | keep cracking         \n [text1, 1351]     just keeping | shizo | overoptimistic also   \n [text1, 1375]       seven days | shizo | wonderful detail      \n [text1, 1398] warmer afternoon | shizo | howeverexercise starts\n\n\nü§î¬øQu√© hemos hecho?\n\nAplicamos la funci√≥n kwic()para extraer las concordancias de las palabras clave. Es decir, el texto anterior y posterior a estos t√©rminos.\nEl par√°metro window = especifica el n√∫mero de palabras que se mostrar√°n antes y despu√©s de cada palabra calve, l oque proporciona un contexto breve que nos permite inspeccionar de manera manual el sentido de cada palabra."
  },
  {
    "objectID": "dataviz.html#topic-modeling",
    "href": "dataviz.html#topic-modeling",
    "title": "An√°lisis y visualizaci√≥n de datos",
    "section": "üß© Topic Modeling",
    "text": "üß© Topic Modeling\nEl an√°lisis de temas, o topic modeling, es una t√©cnica de procesamiento de lenguaje natural que identifica patrones ocultos en un conjunto de textos. Su objetivo es descubrir temas latentes que aparecen recurrentemente, agrupando palabras que suelen aparecer juntas. Cada tema representa un conjunto de palabras que tienden a co-ocurrir, permiti√©ndonos inferir posibles temas o conceptos sin necesidad de leer todo el texto.\nEl modelo de temas que utilizaremos, llamado Latent Dirichlet Allocation (LDA), funciona tratando cada documento como una combinaci√≥n de varios temas, y cada tema como una combinaci√≥n de palabras. As√≠, LDA genera temas representativos del contenido general de los textos, lo cual es √∫til para identificar patrones y estructura tem√°tica en grandes vol√∫menes de texto.\nEn primer lugar vamos refinar nuestra matriz de frecuencias dfm_all eliminando t√©rminos poco frecuentes\n\ndfm_topics &lt;- dfm_trim(dfm_all, min_termfreq = 5)\n\nEn este caso min_termfreq = 5: Elimina t√©rminos que aparecen menos de 5 veces en todo el conjunto de documentos.\nVamos a ver c√≥mo se ha reducido el set de t√©rminos:\n\nlength(dfm_all) # total de t√©rminos\n\n[1] 11440\n\nlength(dfm_topics) # total una vez eliminados los poco frecuentes\n\n[1] 561\n\n\nAhora vamos a usar nuestra nueva matriz de frecuencias dfm_topics para aplicar nuestro modelo de LDA. Importante, aqu√≠ necesitaremos tener instalado el paquete tm y el paquete topicmodels.\n\nlibrary(tm)\nlibrary(topicmodels)\n# Convertir la matriz dfm a formato compatible con LDA\ndfm_topics_lda &lt;- convert(dfm_topics, to = \"topicmodels\")\n\n# Definir el n√∫mero de temas\nnum_topics &lt;- 3  # Ajusta el n√∫mero de temas seg√∫n el an√°lisis deseado\n\n# Ajustar el modelo LDA\nlda_model &lt;- LDA(dfm_topics_lda, k = num_topics, control = list(seed = 1234))\n\n# Obtener las palabras clave de cada tema\nterms(lda_model, 10)  # Muestra las 10 palabras m√°s representativas de cada tema\n\n      Topic 1    Topic 2       Topic 3 \n [1,] \"years\"    \"everyone\"    \"cell\"  \n [2,] \"dont\"     \"like\"        \"night\" \n [3,] \"miss\"     \"theres\"      \"people\"\n [4,] \"can\"      \"convictions\" \"just\"  \n [5,] \"must\"     \"country\"     \"life\"  \n [6,] \"cell\"     \"russia\"      \"can\"   \n [7,] \"idea\"     \"day\"         \"now\"   \n [8,] \"put\"      \"right\"       \"russia\"\n [9,] \"everyone\" \"new\"         \"day\"   \n[10,] \"family\"   \"now\"         \"much\"  \n\n\nü§î¬øQu√© hemos hecho?\n\nLa funci√≥n convert() transforma nuestra matriz de frecuencias en un formato entendible por el paquete topicmodels para poder modelarlo.\nElegimos el n√∫mero de topics con los que queremos trabajar. Esta parte suele ser una decisi√≥n informada tras hacer varias pruebas. En nuestro caso, me quedar√© con 3 topics.\nEntreno al modelo con mis datos con la funci√≥n LDA(). Para permitir reproducibilidad del an√°lisis utilizo el par√°metro control= para establecer una semilla para el generador de n√∫meros aleatorios que emplea el algoritmo."
  }
]