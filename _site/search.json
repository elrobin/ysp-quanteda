[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bienvenidos",
    "section": "",
    "text": "Datos b√°sicos del curso\n\n\n\nMateriales preparados para el curso impartido en la plataforma YoSigo UGR de la Universidad de Granada por Sergio Castro-Cortacero y Nicol√°s Robinson-Garc√≠a el d√≠a 8 de noviembre de 2024."
  },
  {
    "objectID": "index.html#objetivos-de-la-sesi√≥n",
    "href": "index.html#objetivos-de-la-sesi√≥n",
    "title": "Bienvenidos",
    "section": "üéØObjetivos de la sesi√≥n",
    "text": "üéØObjetivos de la sesi√≥n\nEsta web contiene los contenidos del curso de YoSigo UGR Introducci√≥n al an√°lisis de textos con Quanteda. En esta sesi√≥n queremos iniciarte en el fant√°stico mundo del an√°lisis de textos. Para ello vamos a utilizar el lenguaje de programaci√≥n R y el paquete de an√°lisis de textos Quanteda. Para ello vamos a explorar algunas cuestiones b√°sicas para el an√°lisis de textos como es la creaci√≥n de un corpus, el proceso de tokenizaci√≥n y algunos an√°lisis b√°sicos. Al terminar esta sesi√≥n deber√≠as poder hacer lo siguiente:\n\nEntender los fundamentos b√°sicos de miner√≠a de datos.\nSer capaz de importar y tokenizar un texto empleando el paquete de R quanteda.\nRealizar un an√°lisis pr√°ctico utilizando un corpus de texto real."
  },
  {
    "objectID": "index.html#antes-de-comenzar",
    "href": "index.html#antes-de-comenzar",
    "title": "Bienvenidos",
    "section": "ü´∂Antes de comenzar",
    "text": "ü´∂Antes de comenzar\nPara poder repetir las cosas que vamos a hacer en este curso y poder realizar tus propios an√°lisis, deber√°s de tener instalado en tu ordenador los siguientes programas (inst√°lalos en el orden que te indico):\n\nR. Este es el lenguaje programaci√≥n que emplearemos. Se trata de un lenguaje abierto y gratuito desarrollado espec√≠ficamente para el an√°lisis estad√≠stico. Aunque como con cualquier otro lenguaje de programaci√≥n, es muy flexible y puede emplearse para diferentes funciones. Cuenta con una amplia comunidad de usuarios que crean y mantienen sus funcionalidades creando lo que se conoce como paquetes o librer√≠as.\nRStudio. Se trata de una interfaz de R que facilita algunas operaciones. Aunque en principio, R puede emplearse sin interfaz, RStudio incluye los principales paquetes adicionales as√≠ como algunas funcionalidades que pueden ser muy √∫tiles para mantenernos cuerdos al adentrarnos en el mundo de la programaci√≥n.\n\n\nüí° Aqu√≠ te daremos el c√≥digo muy mascadito, pero si no tienes experiencia programando con R y deseas conocer las bases de su gram√°tica, te recomiendo los siguientes recursos:\n\nEl curso de yosigo Introducci√≥n pr√°ctica a la ciencia de datos en R ‚Äì REDUX by Wenceslao Arroyo-Machado\nEl libro online R for Data Science by Hadley Wickham, uno de los m√°ximos exponentes en R"
  },
  {
    "objectID": "ysp_tutorial.html",
    "href": "ysp_tutorial.html",
    "title": "An√°lisis de textos",
    "section": "",
    "text": "El an√°lisis de textos es el proceso de convertir grandes vol√∫menes de texto en datos estructurados que nos permitan identificar patrones, tendencias, y relaciones dentro de ese texto. Es una t√©cnica fundamental en campos como las ciencias sociales, humanidades digitales, marketing, y pol√≠tica, ya que permite explorar y entender la informaci√≥n contenida en textos de manera cuantitativa.\nA medida que el volumen de informaci√≥n textual digitalizada ha crecido, la necesidad de analizar textos de forma sistem√°tica tambi√©n ha aumentado. Herramientas como quanteda permiten a los investigadores y analistas transformar textos en datos estructurados y cuantificables."
  },
  {
    "objectID": "ysp_tutorial.html#introducci√≥n-al-an√°lisis-cuantitativo-de-textos",
    "href": "ysp_tutorial.html#introducci√≥n-al-an√°lisis-cuantitativo-de-textos",
    "title": "An√°lisis de textos",
    "section": "",
    "text": "El an√°lisis de textos es el proceso de convertir grandes vol√∫menes de texto en datos estructurados que nos permitan identificar patrones, tendencias, y relaciones dentro de ese texto. Es una t√©cnica fundamental en campos como las ciencias sociales, humanidades digitales, marketing, y pol√≠tica, ya que permite explorar y entender la informaci√≥n contenida en textos de manera cuantitativa.\nA medida que el volumen de informaci√≥n textual digitalizada ha crecido, la necesidad de analizar textos de forma sistem√°tica tambi√©n ha aumentado. Herramientas como quanteda permiten a los investigadores y analistas transformar textos en datos estructurados y cuantificables."
  },
  {
    "objectID": "ysp_tutorial.html#para-qu√©-se-utiliza-el-an√°lisis-de-textos",
    "href": "ysp_tutorial.html#para-qu√©-se-utiliza-el-an√°lisis-de-textos",
    "title": "An√°lisis de textos",
    "section": "ü§î ¬øPara qu√© se utiliza el an√°lisis de textos?",
    "text": "ü§î ¬øPara qu√© se utiliza el an√°lisis de textos?\n\n\n\n\n\n\n\nAn√°lisis de sentimiento\nEvaluar el tono emocional de textos, como rese√±as o comentarios en redes sociales, para conocer las opiniones y actitudes de los usuarios.\n\n\nAn√°lisis de tendencias\nIdentificar la frecuencia de palabras o frases clave y su cambio a lo largo del tiempo.\n\n\nAn√°lisis del discurso\nExplorar c√≥mo ciertos temas o ideas son representados en la sociedad y su evoluci√≥n temporal\n\n\nClasificaci√≥n autom√°tica de textos\nOrganizar categ√≥ricamente textos y fragmentos de textos a trav√©s de t√©cnicas como topic modeling o similares"
  },
  {
    "objectID": "ysp_tutorial.html#quanteda-como-herramienta-para-el-an√°lisis-de-datos",
    "href": "ysp_tutorial.html#quanteda-como-herramienta-para-el-an√°lisis-de-datos",
    "title": "An√°lisis de textos",
    "section": "‚úçÔ∏è Quanteda como herramienta para el an√°lisis de datos",
    "text": "‚úçÔ∏è Quanteda como herramienta para el an√°lisis de datos\nQuanteda es un paquete de R dise√±ado espec√≠ficamente para el an√°lisis cuantitativo de textos. Su valor principal radica en que ofrece una forma r√°pida, flexible y eficiente de transformar textos en datos estructurados, permitiendo a los investigadores trabajar con grandes vol√∫menes de texto de manera sistem√°tica. Quanteda es ideal para el procesamiento inicial y an√°lisis exploratorio de textos."
  },
  {
    "objectID": "ysp_tutorial.html#por-qu√©-quanteda",
    "href": "ysp_tutorial.html#por-qu√©-quanteda",
    "title": "An√°lisis de textos",
    "section": "‚ÅâÔ∏è ¬øPor qu√© quanteda?",
    "text": "‚ÅâÔ∏è ¬øPor qu√© quanteda?\n\n\n\n\n\n\n\nFacilidad en la preprocesamiento\nPermite realizar tareas como tokenizaci√≥n y limpieza de datos de forma r√°pida y eficiente.\n\n\nAn√°lisis de frecuencias\nFacilita el c√°lculo de frecuencias de t√©rminos en un corpus, ideal para identificar palabras clave y patrones.\n\n\nExploraci√≥n de contexto (KWIC)\nLa funci√≥n KWIC permite analizar palabras en su contexto, proporcionando insights sobre el uso de ciertos t√©rminos.\n\n\nAn√°lisis de sentimiento y temas\nPermite aplicar diccionarios de sentimiento y realizar an√°lisis b√°sico de temas mediante co-ocurrencia de palabras.\n\n\nFlexibilidad y escalabilidad\nOptimizado para manejar grandes vol√∫menes de texto y se integra f√°cilmente con otros paquetes para an√°lisis avanzados."
  },
  {
    "objectID": "dataprocessing.html",
    "href": "dataprocessing.html",
    "title": "Carga y procesamiento de textos",
    "section": "",
    "text": "Para seguir este tutorial por primera vez, deber√°s instalar una serie de paquetes1 que emplearemos: quanteda, readtext, dplyr y stringr. Esto se hace a trav√©s del comando: install.packages():\n\ninstall.packages(c(\"quanteda\", \"readtext\", \"dplyr\", \"stringi\"))\n\n\n\n\nquanteda\n\nPaquete de an√°lisis de textos, incluyendo tokenizaci√≥n, conteo y limpieza de textos\n\nreadtext\n\nPermite importar archivos de texto en varios formatos, facilitando la carga de datos\n\ndplyr\n\nHerramienta para manipulaci√≥n y transformaci√≥n de datos, √∫til para filtrar y organizar datos\n\nstringi\n\nConjunto de funciones para trabajar con texto, especialmente √∫til para limpieza y manejo de expresiones regulares.2\n\n\n¬øSab√≠as qu√©‚Ä¶? Ô∏èü§ì‚òù\n\nLa tokenizaci√≥n es el proceso de dividir un texto en unidades m√°s peque√±as llamdas tokens. Estas unidades pueden ser palabras, s√≠mbolos, frases o incluso caracteres dependiendo del tipo de an√°lisis que se vaya a realizar. En quanteda consideramos la palabra como la unidad m√≠nima de trabajo. Imag√≠nate que tienes la siguiente oraci√≥n:\n‚ÄúHola, ¬øc√≥mo est√°s?‚Äù\nLa tokenizaci√≥n de esta frase podr√≠a dar como resultado los siguientes tokens: ‚ÄúHola‚Äù, ‚Äú¬ø‚Äù, ‚Äúc√≥mo‚Äù, ‚Äúest√°s‚Äù, ‚Äú?‚Äù\nEsto es especialmente √∫til cuando estamos trabajando con estudios relacionados con frecuencias de palabras."
  },
  {
    "objectID": "dataprocessing.html#preparando-los-paquetes",
    "href": "dataprocessing.html#preparando-los-paquetes",
    "title": "Carga y procesamiento de textos",
    "section": "",
    "text": "Para seguir este tutorial por primera vez, deber√°s instalar una serie de paquetes1 que emplearemos: quanteda, readtext, dplyr y stringr. Esto se hace a trav√©s del comando: install.packages():\n\ninstall.packages(c(\"quanteda\", \"readtext\", \"dplyr\", \"stringi\"))\n\n\n\n\nquanteda\n\nPaquete de an√°lisis de textos, incluyendo tokenizaci√≥n, conteo y limpieza de textos\n\nreadtext\n\nPermite importar archivos de texto en varios formatos, facilitando la carga de datos\n\ndplyr\n\nHerramienta para manipulaci√≥n y transformaci√≥n de datos, √∫til para filtrar y organizar datos\n\nstringi\n\nConjunto de funciones para trabajar con texto, especialmente √∫til para limpieza y manejo de expresiones regulares.2\n\n\n¬øSab√≠as qu√©‚Ä¶? Ô∏èü§ì‚òù\n\nLa tokenizaci√≥n es el proceso de dividir un texto en unidades m√°s peque√±as llamdas tokens. Estas unidades pueden ser palabras, s√≠mbolos, frases o incluso caracteres dependiendo del tipo de an√°lisis que se vaya a realizar. En quanteda consideramos la palabra como la unidad m√≠nima de trabajo. Imag√≠nate que tienes la siguiente oraci√≥n:\n‚ÄúHola, ¬øc√≥mo est√°s?‚Äù\nLa tokenizaci√≥n de esta frase podr√≠a dar como resultado los siguientes tokens: ‚ÄúHola‚Äù, ‚Äú¬ø‚Äù, ‚Äúc√≥mo‚Äù, ‚Äúest√°s‚Äù, ‚Äú?‚Äù\nEsto es especialmente √∫til cuando estamos trabajando con estudios relacionados con frecuencias de palabras."
  },
  {
    "objectID": "dataprocessing.html#volcando-los-archivos-de-texto-a-rstudio",
    "href": "dataprocessing.html#volcando-los-archivos-de-texto-a-rstudio",
    "title": "Carga y procesamiento de textos",
    "section": "üìÑ‚û°Ô∏èüñ•Ô∏èVolcando los archivos de texto a RStudio",
    "text": "üìÑ‚û°Ô∏èüñ•Ô∏èVolcando los archivos de texto a RStudio\nVamos a vincular el archivo de¬†texto en la aplicaci√≥n de RStudios. Para ello vamos a aplicar los dos paquetes que hemos instalado anteriormente¬†con el comando library(\"name\")\n\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\n\nEs importante resaltar que, si no llamamos antes el paquete, los comandos que introduzcamos despu√©s no funcionar√°n o nos dar√°n error. Aseg√∫rate de cargar siempre la librer√≠a antes de empezar a trabajar.\nUna vez cargados, el programa estar√° listo para leer nuestro archivo de texto. La formula que vamos a escribir para decirle a quanteda que archivo analizar ser√° el siguiente:\n\ndata_char_navalny &lt;- as.character(readtext(\"NAVALNY.txt\"))\nnames(data_char_navalny) &lt;- \"navalny\"\n\n‚ùóATENCI√ìN: Si por alguna raz√≥n hiciesemos alg√∫n cambio en el contenido del archivo, deberemos de aplicar el paso anterior de nuevo. Cuando cargamos un archivo en R, se guarda una copia y cualquier cambio en el original no se refleja autom√°ticamente.\n\nü§î¬øQu√© significa esta funci√≥n?\nVamos a desgranar este prompt para que pueda entenderse m√°s facil:\n\ndata_char_navalny: La variable data_char_navalny es un objeto[2] en R que almacena el texto como una cadena de caracteres (character vector).\nas.character(): Convierte el objeto cargado en un formato de texto sencillo, que es m√°s f√°cil de manejar para el an√°lisis.\nreadtext: Aqu√≠ estamos empleando una funci√≥n expec√≠fica del paquete readtext. Si no hubi√©semos cargado el paquete anteriormente, podr√≠amos invocarlo espec√≠ficamente para activar esta funci√≥n con la notaci√≥n paquete::funci√≥n. Aqu√≠, lo que le estamos diciendo a RStudio es que, del paquete readtext, aplique espec√≠ficamente la funci√≥n lectura que casualmente tambi√©n se llama readtext .\nUn problema muy com√∫n que puede surgir a la hora de introducir la URL de la ubicaci√≥n del archivo es expresarlo con barras laterales izquierdas ‚Äù \\ ‚Äú, tal y como viene en la barra de direcci√≥n del explorador de archivos de Windows, en vez de la derecha‚Äù / ‚Äú. Si tienes problemas leer tu .txt ¬°prueba con hacer este cambio!\nnames(data_char_navalny) &lt;- \"navalny\": Asigna un nombre al objeto que contiene el texto, facilitando su identificaci√≥n en futuros an√°lisis.\n\n¬øSab√≠as qu√©‚Ä¶? Ô∏èü§ì‚òù\n\n[2] En R hablamos de objetos para referirnos a los contenedores donde almacenamos datos e informaci√≥n. En el caso anterior, el objeto data_char_navalny almacena el texto plano que vamos a utilizar.\nExisten distintos objetos con diferentes datos almacenados: matrices, n√∫meros, listas jerarquizadas, as√≠ como un sin fin de combinaciones. A lo largo de este caso pr√°ctico trabajaremos con ellos para gestionar m√°s facilmente el an√°lisis cuantitativo.\n\n\n\nüîç Comprobaciones\nEn el an√°lisis cuantitativo toda precauci√≥n es poca , as√≠ que vamos a realizar una serie de comprobaciones para verificar que el texto ha sido leido por el programa. Para ello, vamos a pedirle a RStudio que nos devuelva los primeros 75 caracteres de nuestro archivo NALVANY.txt mediante el paquete: stringi.\n\nlibrary(\"stringi\") #Primero llamamos al paquete\nstri_sub(data_char_navalny, 1, 75) #Aplicamos la funci√≥n para que nos devuelva los caracteres situados entre la posici√≥n 1 y 75.\n\nSi hemos hecho los pasos bien, tendr√©is que haber recibir este texto de vuelta:\n[1] \"2022\\nJanuary 17th\\nExactly one year ago today I came home, to Russia.\\nI didn\""
  },
  {
    "objectID": "dataprocessing.html#discriminaci√≥n-del-texto",
    "href": "dataprocessing.html#discriminaci√≥n-del-texto",
    "title": "Carga y procesamiento de textos",
    "section": "üóÉÔ∏èDiscriminaci√≥n del texto",
    "text": "üóÉÔ∏èDiscriminaci√≥n del texto"
  },
  {
    "objectID": "dataprocessing.html#acotando-nuestro-espacio-de-trabajo",
    "href": "dataprocessing.html#acotando-nuestro-espacio-de-trabajo",
    "title": "Carga y procesamiento de textos",
    "section": "üóÉÔ∏èAcotando nuestro espacio de trabajo",
    "text": "üóÉÔ∏èAcotando nuestro espacio de trabajo\nNuestro siguiente objetivo es seleccionar qu√© partes del texto vamos aplicar el an√°lisis cuantitativo. Puede que nuestro foco de inter√©s sea algun apartado concreto de nuestro material, por lo que vamos a crear un objeto que albergue un rango determinado dentro de nuestro ‚Äú‚Äú.txt . Con esto nos quitaremos toda la informaci√≥n innecesaria que puede ensuciar nuestros resultados.\nEl proceso que vamos a realizar a continuaci√≥n es muy √∫til cuando los archivos que manejamos tienen ligados metadatos. Normalmente, esta metadata suele ser m√°s un dolor de cabeza que otra cosa y es recomendable realizar una limpieza previa para que esos datos no se mezclen con el contenido de nuestro an√°lisis.\nEn este apartado seguiremos trabajando con el paquete stringi 1\n\nPASO 1: Asignaci√≥n de los rangos del texto\nPara crear el objeto que albergue el rango de texto a analizar deberemos empezar indicando donde empieza y termina nuestra selecci√≥n. Para ello, crearemos dos valores de posici√≥n: start_v y end_v, donde start_ ser√°: ‚Äú2023, January 12th‚Äù y end_v ‚ÄúFIN‚Äù.\nLocalizado el rango que queremos, la forma de expresarlo en el programa ser√≠a el siguiente:\n\n(start_v &lt;- stri_locate_first_fixed(data_char_NALVANY, \"2023\\nJanuary 12th\")[1])\n\n\n(end_v &lt;- stri_locate_last_fixed(data_char_NALVANY, \"FIN\")[1])\n\nSi lo hemos aplicado bien, la funci√≥n deber√≠a de devolver los siguientes resultados\n\nPara start_value\n[1] 23653\n\n\n\nPara end_value\n[1] 44141\n\n\n\nü§î¬øQu√© significa esta expresi√≥n?\n\nTanto start_v como end_v son nombres que hemos asignado a la posici√≥n espec√≠ficas del texto. En s√≠, no significan nada. Solo decimos, a trav√©s de ‚Äú&lt;-‚Äù que dichos nombres albergan una funci√≥n de posicionamiento.\nLas funciones del paquete stringi: stri_locate_first_fixed y stri_locate_last_fixed buscan y encuentran la primera coincidencia del valor entrecomillado que precede a nuestro objeto data_char_NALVANY\nEl [1] es un indicador que le estamos dando a la funci√≥n para que escoja la primera posici√≥n donde aparezca el texto que hayamos escogido.\nAs√≠, cuando vemos devuelto las respuestas [1] 23653 y [1] 44141 quiere decir que para start_v y end_v est√° asignado el primer valor donde aparece dichas expresiones , localizadas por primera vez en la posici√≥n 23653 y 44141 de nuestro texto.\n\n\n\nPASO 2, creando nuestro objeto\nCreado nuestro punto de inicio y final de nuestra zona de trabajo, haremos un objeto que alberge dicho rango. Lo llamaremos: diary_v\n\ndiary_v &lt;- stri_sub(data_char_NALVANY, start_v, end_v)\nlength(diary_v)\n\nAl iniciar el c√≥digo el valor que os ha tenido que recuperar, adem√°s de almacenar el objeto en la pesta√±a Environment de RStuido, es:\n[1] 1\n\n\nü§î¬øQu√© significa esta expresi√≥n?\n\ndiary_v es el nombre del objeto que almacena la funci√≥n que ha sido asignada. En este caso, a trav√©s de stri_sub, estamos extrayendo una parte del texto data_char_NALVANY . A diferencia del caso anterior, aqu√≠ le estamos pidiendo que, en vez de que recuper un n√∫mero detemrinado de caracteres, escoja todos los que hay comprendidos entre las posiciones que hemos dado a start_v y a end_v. Con esto nos aseguramos que el objeto diary_v siempre trabaje en los rangos que nos interesa analizar.\nlegth(diary_v) es una expresi√≥n que usamos para comprobar cuandos valores existen en nuestro objeto. Es una forma de asegurarnos de que nuestro objeto solo tiene un vector y no es un conjunto de fragmentos de texto. Por eso, al introducirlo, el programa nos devuelve el valor 1 porque solo hay 1 valor dentro de nuestro objeto."
  },
  {
    "objectID": "dataprocessing.html#a-la-tokenizaci√≥n",
    "href": "dataprocessing.html#a-la-tokenizaci√≥n",
    "title": "Carga y procesamiento de textos",
    "section": "üóø ¬°A la tokenizaci√≥n!",
    "text": "üóø ¬°A la tokenizaci√≥n!\nYa tenemos nuestro texto bien organizado y estructurado. Toca dividir a√∫n m√°s y limpiar. Para ello vamos a hacer lo siguiente:\n\nConvertimos el texto en min√∫sculas y eliminaos los signos de puntuaci√≥n\nTokenizamos el texto dividi√©ndolo en palabras. De manera que nuestra unidad de an√°lisis ser√° la palabra4.\nEliminamos todas las palabras vac√≠as5 para quedarnos s√≥lo con aquellas relevantes para nuestro an√°lisis.\n\n\nPASO 1: Texto en min√∫scula y puntuaci√≥n fuera\nPara asegurarnos que palabras id√©nticas no se traten como diferentes por su formato, converitmos todo el texto a min√∫sculas y eliminamos puntuaci√≥n.\n\n# Convertir cada entrada diaria a min√∫sculas y eliminar signos de puntuaci√≥n\nyearly_entries &lt;- lapply(yearly_entries, function(year) {\n  year$entries &lt;- lapply(year$entries, function(entry) {\n    # Convertir el texto a min√∫sculas y eliminar puntuaci√≥n\n    entry &lt;- char_tolower(entry)\n    entry &lt;- gsub(\"[[:punct:]]\", \"\", entry)\n    entry\n  })\n  year  # Devolver la lista de a√±o modificada\n})\n\nü§î¬øQu√© hemos hecho?\n\nLa funci√≥n char_tolower() convierte el texto en min√∫sculas.\nLa funci√≥n gsub() sustituye un patr√≥n de texto por otro. En nuestro caso, le hemos pedido que busque cualquier signo de puntuaci√≥n empleando la expresi√≥n regular [[:punct:]] y la reemplace por nada.\nDespu√©s hemos pedido que incluya estos cambios nuevamente en nuestro objeto yearly_entries.\n\nSi observas las primeras l√≠neas de una entrada, ver√°s que todo ha funcionado tal y como esper√°bamos:\n\n\n[1] \"january 12th\"                                                                   \n[2] \"in my two years behind bars my only truly original story is the one about the\"  \n[3] \"psycho everything else has been told and described numerous times if you\"       \n[4] \"open any book by a soviet dissident there will be endless stories of punishment\"\n[5] \"cells hunger strikes violence provocations lack of medical care nothing new\"    \n\n\n\n\nPASO 2: Tokenizaci√≥n\nPara que el programa pueda analizar y realizar manipulaciones sobre las palabras de forma individualizada, vamos a convertir a cada una de ellas en peque√±os valores que llamamos tokens.\n\n# Tokenizar cada entrada diaria dentro de cada a√±o\nyearly_entries &lt;- lapply(yearly_entries, function(year) {\n  year$entries &lt;- lapply(year$entries, function(entry) {\n    tokens(entry, what = \"word\")\n  })\n  year  # Devolver la lista de a√±o modificada\n})\n\nü§î¬øQu√© hemos hecho?\n\nSiguiendo la misma estructura de la vez anterior, hemos incorporado la funci√≥n tokens() y lo hemos aplicado al objeto entry\nAdem√°s, hemos utilizado el par√°metro what= en el que indicamos el nivel. En nuestro caso tokenizamos por palabras. Otras opciones son por caracteres (character) y frases (sentence).\n\nAhora en lugar de contener un listado de filas por entrada, lo que tengo es una bolsa de palabras:\n\n\nTokens consisting of 3 documents.\ntext1 :\n[1] \"january\" \"12th\"   \n\ntext2 :\n [1] \"in\"       \"my\"       \"two\"      \"years\"    \"behind\"   \"bars\"    \n [7] \"my\"       \"only\"     \"truly\"    \"original\" \"story\"    \"is\"      \n[ ... and 4 more ]\n\ntext3 :\n [1] \"psycho\"     \"everything\" \"else\"       \"has\"        \"been\"      \n [6] \"told\"       \"and\"        \"described\"  \"numerous\"   \"times\"     \n[11] \"if\"         \"you\"       \n\n\n\n\nPASO 3: Eliminaci√≥n de palabras vac√≠as\nPara centrarnos en las palabras significativas, eliminamos las palabras vac√≠as (stopwords), que suelen ser t√©rminos comunes y poco informativos, como ‚Äúel‚Äù, ‚Äúde‚Äù, ‚Äúy‚Äù. Esto permite que el an√°lisis se centre en t√©rminos con m√°s contenido sem√°ntico.\n\n# Eliminar palabras vac√≠as en ingl√©s en cada entrada diaria\nyearly_entries &lt;- lapply(yearly_entries, function(year) {\n  year$entries &lt;- lapply(year$entries, function(entry) {\n    tokens_remove(entry, pattern = stopwords(\"en\")) \n  })\n  year  # Devolver la lista de a√±o modificada\n})\n\nü§î¬øQu√© hemos hecho?\n\nAqu√≠ empleamos la funci√≥n tokens_remove() otra vez aplicada al objeto entry, en este caso empleamos el par√°metro pattern = para indicar que eliminaremos los tokens que representen palabras vac√≠as, en par√©ntesis incluimos la lengua a trav√©s de su c√≥digo ISO, en nuestro caso el ingl√©s. Aqu√≠ tienes el listado completo de idiomas. El paquete stopwords permite asimismo crear y/o a√±adir tus propias palabras vac√≠as.\n\nF√≠jate c√≥mo, en comparaci√≥n con el fragmento anterior, se han eliminado palabras como ‚Äúin‚Äù, ‚Äúmy‚Äù u ‚Äúonly‚Äù.\n\n\nTokens consisting of 3 documents.\ntext1 :\n[1] \"january\" \"12th\"   \n\ntext2 :\n[1] \"two\"      \"years\"    \"behind\"   \"bars\"     \"truly\"    \"original\" \"story\"   \n[8] \"one\"     \n\ntext3 :\n[1] \"psycho\"     \"everything\" \"else\"       \"told\"       \"described\" \n[6] \"numerous\"   \"times\"     \n\n\n\n\n\n\n\n\nComo habr√°s notado, es posible juntar todos estos pasos en uno s√≥lo. Aqu√≠ lo mostramos por trozos para que vayas comprendiendo el proceso, pero podr√≠amos hacer todo esto de una vez.\n\n\n\n¬øSab√≠as qu√©‚Ä¶? Ô∏èü§ì‚òù\n\nEn ingl√©s, es posible que el texto venga acompa√±ado de ap√≥strofes como en los casos de don't y he's. Aqu√≠, quanteda no tomar√° las 't ni las 's como elementos aislados, sino que lo mantendr√° unida a la palabra para respetar el significado original."
  },
  {
    "objectID": "dataprocessing.html#las-utilidades",
    "href": "dataprocessing.html#las-utilidades",
    "title": "Carga y procesamiento de textos",
    "section": "‚öôÔ∏è Las utilidades",
    "text": "‚öôÔ∏è Las utilidades\nAhora que tenemos todo nuestro texto listo para trabajar, podemos realizar operaciones sencillas, tales como:\n\nComprobar cuantas veces se repite una palabra y su localizaci√≥n.\n\nEsto es especialmente util para realizar an√°lisis de frecuencia o de sentimientos al conocer el contexto donde se menciona esa palabra.\n\nwhich(nalvany_word_v[[\"text1\"]] == \"putin\") |&gt;\n+     head()\n\n[1] 1735 2771\n\nLocalizar una palabra exacta a trav√©s de su n√∫mero de caracter.\n\nComo sabemos el n√∫mero total de caracteres que tiene gracias a la funci√≥n ntoken podemos recuperar cualquier elemento con solo introducir un valor concreto.\nEsto nos podr√≠a ayudar para saber qu√© palabra se encuentra en una posici√≥n determinada para estudiar el contexto en el que aparece.\n\nnalvany_word_v[[\"text1\"]][3666] \n\n[1] \"it\""
  },
  {
    "objectID": "dataprocessing.html#footnotes",
    "href": "dataprocessing.html#footnotes",
    "title": "Carga y procesamiento de textos",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\n\n\n\n\n\nR es un lenguaje de programaci√≥n abierto y colaborativo que sigue una estructura totalmente descentralizada. Cuando instalamos R por primera vez, s√≥lo instalamos sus funcionalidades b√°sicas. Todas aquellas funcionalidades adicionales llevadas a cabo por terceras personas deben instalarse en lo que se denominan paquetes o libraries en ingl√©s.\nCada vez que vayas a emplear un paquete, debes cargarlo, por defecto, cada vez que abres R estos paquetes no est√°n cargados.\n\n\n\n‚Ü©Ô∏é\n\n\n\n\n\n\nUna expresi√≥n regular es un patr√≥n de b√∫squeda utilizado para manipular texto espec√≠fico en una cadena. Facilita tareas como eliminar caracteres no deseados o extraer informaci√≥n espec√≠fica (e.g., fechas o n√∫meros). Es especialmente √∫til en la limpieza y preprocesamiento de datos textuales.\n\n\n\n‚Ü©Ô∏é\n\n\n\n\n\n\nUna lista es una estructura de datos qeu puede contener elementos de diferentes tipos (num√©rico, caract√©res, vectores o incluso otras listas) en un solo objecto. Cada elemento en una lista se puede acceder de forma individual usando √≠ndices. Esto se hace utilizando corchetes dobles. Por ejemplo si queremos ver el segundo elemento de la lista lista, lo indicaremos as√≠: lista[[2]].\n\n\n\n‚Ü©Ô∏é\n\n\n\n\n\n\nAqu√≠ es importante diferenciar entre an√°lisis textual y un an√°lisis sem√°ntico. En el an√°lisis de textos examinamos cuestiones como la frecuencia de las palabras, patrones o estructura, sin considerar el significado de cada palabra. Se tratar√≠a de un paso previo al an√°lisis sem√°ntico donde nos centramos en el significado y el contexto de las palabras.\n\n\n\n‚Ü©Ô∏é\n\n\n\n\n\n\nLas palabras vac√≠as son t√©rminos comunes (como ‚Äúel‚Äù, ‚Äúde‚Äù, ‚Äúy‚Äù) que suelen aparecer con mucha frecuencia en el texto, pero aportan poco significado o valor informativo al an√°lisis. Estas palabras se eliminan generalmente para centrar el an√°lisis en los t√©rminos m√°s relevantes.\n\n\n\n‚Ü©Ô∏é"
  },
  {
    "objectID": "dataprocessing.html#importaci√≥n-de-datos-de-texto",
    "href": "dataprocessing.html#importaci√≥n-de-datos-de-texto",
    "title": "Carga y procesamiento de textos",
    "section": "üìÑ‚û°Ô∏èüñ•Ô∏èImportaci√≥n de datos de texto",
    "text": "üìÑ‚û°Ô∏èüñ•Ô∏èImportaci√≥n de datos de texto\nVamos a vincular el archivo de¬†texto en la aplicaci√≥n de RStudios. Para ello vamos a cargar los paquetes que hemos instalado anteriormente¬†con el comando library(\"name\")\n\nlibrary(quanteda)\nlibrary(readtext)\nlibrary(dplyr)\nlibrary(stringi)\n\nEs importante resaltar que, si no llamamos antes el paquete, los comandos que introduzcamos despu√©s no funcionar√°n o nos dar√°n error. Aseg√∫rate de cargar siempre la librer√≠a antes de empezar a trabajar.\nUna vez cargados, el programa estar√° listo para leer nuestro archivo de texto. La formula que vamos a escribir para decirle a quanteda que archivo analizar ser√° el siguiente:\n\nnavalny_raw &lt;- as.character(readtext(\"NAVALNY.txt\"))\nnames(navalny_raw) &lt;- \"navalny\"\n\n‚ùóATENCI√ìN: Si por alguna raz√≥n hiciesemos alg√∫n cambio en el contenido del archivo, deberemos de aplicar el paso anterior de nuevo. Cuando cargamos un archivo en R, se guarda una copia y cualquier cambio en el original no se refleja autom√°ticamente.\nü§î¬øQu√© hemos hecho?\nEste comando carga el archivo NAVALNY.txt en el objeto navalny_raw, el cual contiene el contenido del texto. Vamos a desgranar este prompt para que pueda entenderse m√°s facil:\n\nnavalny_raw es un objeto en R que almacena el texto como una cadena de caracteres (character vector). En R hablamos de objetos para referirnos a los contenedores donde almacenamos datos e informaci√≥n. En el caso anterior, el objeto data_char_navalny almacena el texto plano que vamos a utilizar. Existen distintos objetos con diferentes datos almacenados: matrices, n√∫meros, listas jerarquizadas, as√≠ como un sin fin de combinaciones. A lo largo de este caso pr√°ctico trabajaremos con ellos para gestionar m√°s facilmente el an√°lisis cuantitativo.\n&lt;- emula a una flecha y b√°sicamente indica la direcci√≥n de la acci√≥n. Al objeto navalny_raw estamos aplic√°ndole una funci√≥n\nas.character(): Se trata de la funci√≥n que estamos aplicando. Esta funci√≥n convierte todo lo que se contiene dentro de ella en car√°cter. En R toda funci√≥n viene seguida por unos par√©ntesis dentro de los cu√°les se incluyen los par√°metros de dicha funci√≥n. Si no hay contenido, se aplican los par√°metros que la funci√≥n trae por defecto.\nreadtext: Aqu√≠ estamos empleando una funci√≥n expec√≠fica del paquete readtext. Si no hubi√©semos cargado el paquete anteriormente, podr√≠amos invocarlo espec√≠ficamente para activar esta funci√≥n con la notaci√≥n paquete::funci√≥n. Aqu√≠, lo que le estamos diciendo a RStudio es que, del paquete readtext, aplique espec√≠ficamente la funci√≥n lectura que casualmente tambi√©n se llama readtext . Dentro indicamos entrecomillada la ruta del archivo a importar. Un problema muy com√∫n que puede surgir a la hora de introducir la URL de la ubicaci√≥n del archivo es expresarlo con barras laterales izquierdas ‚Äù \\ ‚Äú, tal y como viene en la barra de direcci√≥n del explorador de archivos de Windows, en vez de la derecha‚Äù / ‚Äú. Si tienes problemas leer tu .txt ¬°prueba con hacer este cambio!\nnames(navalny_raw) &lt;- \"navalny\": Asigna un nombre al objeto que contiene el texto, facilitando su identificaci√≥n en futuros an√°lisis.\n\n\nüîç Comprobaciones\nEn el an√°lisis cuantitativo toda precauci√≥n es poca. Vamos a verificar que el texto ha sido le√≠do por el programa. Usaremos el paquete stringi para ver los primeros 75 caracteres de nuestro archivo y confirmar que los datos se cargaron bien.\n\n# Comprobar los primeros 75 caracteres del texto\nstri_sub(navalny_raw, 1, 75)\n\n[1] \"2022\\nJanuary 17th\\nExactly one year ago today I came home, to Russia.\\nI didn\"\n\n\nSi hemos hecho los pasos bien, tendr√©is que haber recibir este texto de vuelta:\n[1] \"2022\\nJanuary 17th\\nExactly one year ago today I came home, to Russia.\\nI didn\""
  },
  {
    "objectID": "dataprocessing.html#acotando-el-texto-a-analizar",
    "href": "dataprocessing.html#acotando-el-texto-a-analizar",
    "title": "Carga y procesamiento de textos",
    "section": "üóÉÔ∏èAcotando el texto a analizar",
    "text": "üóÉÔ∏èAcotando el texto a analizar\nNuestro siguiente objetivo es seleccionar qu√© partes del texto vamos aplicar el an√°lisis cuantitativo. Puede que nuestro foco de inter√©s sea algun apartado concreto de nuestro material, por lo que vamos a crear un objeto que albergue un rango determinado dentro de nuestro fichero txt . Con esto, nos quitaremos toda la informaci√≥n innecesaria que puede ensuciar nuestros resultados.\nEl proceso que vamos a realizar a continuaci√≥n es muy √∫til cuando los archivos que manejamos tienen ligados metadatos. Normalmente, esta metadata suele ser m√°s un dolor de cabeza que otra cosa y es recomendable realizar una limpieza previa para que esos datos no se mezclen con el contenido de nuestro an√°lisis. En este apartado seguiremos trabajando con el paquete stringi.\nSi el texto contiene secciones que no necesitamos para el an√°lisis, podemos filtrarlas o limpiarlas en esta etapa.\n\nPASO 1: Identificaci√≥n de comienzo y fin del texto\nPara crear el objeto que albergue el rango de texto a analizar deberemos empezar indicando donde empieza y termina nuestra selecci√≥n. Para ello, crearemos dos valores de posici√≥n: start_v y end_v, donde start_ ser√°: ‚Äú2023, January 12th‚Äù y end_v ‚ÄúAlexei Navalny died‚Äù.\nLocalizado el rango que queremos, la forma de expresarlo en el programa ser√≠a el siguiente:\n\n(start_v &lt;- stri_locate_first_fixed(navalny_raw, \"2023\\nJanuary 12th\")[1])\n\n[1] 23654\n\n\n\n(end_v &lt;- stri_locate_last_fixed(navalny_raw, \"Alexei Navalny died\")[1])\n\n[1] 44099\n\n\nSi lo hemos aplicado bien, la funci√≥n deber√≠a de devolver los siguientes resultados\n\nPara start_value\n[1] 23653\n\n\n\nPara end_value\n[1] 44099\n\nü§î¬øQu√© hemos hecho?\n\nTanto start_v como end_v son nombres que hemos asignado a la posici√≥n espec√≠ficas del texto. En s√≠, no significan nada. Solo decimos, a trav√©s de ‚Äú&lt;-‚Äù que dichos nombres albergan una funci√≥n de posicionamiento.\nLas funciones del paquete stringi: stri_locate_first_fixed y stri_locate_last_fixed buscan y encuentran la primera coincidencia del valor entrecomillado que precede a nuestro objeto data_char_NALVANY\nEl [1] es un indicador que le estamos dando a la funci√≥n para que escoja la primera posici√≥n donde aparezca el texto que hayamos escogido.\nAs√≠, cuando vemos devuelto las respuestas [1] 23653 y [1] 44141 quiere decir que para start_v y end_v est√° asignado el primer valor donde aparece dichas expresiones , localizadas por primera vez en la posici√≥n 23653 y 44141 de nuestro texto.\n\n\n\nPASO 2: Nuevo objeto\nCreado nuestro punto de inicio y final de nuestra zona de trabajo, haremos un objeto que alberge dicho rango. Lo llamaremos: navalny_fix\n\nnavalny_fix &lt;- stri_sub(navalny_raw, start_v, end_v)\nlength(navalny_fix)\n\n[1] 1\n\n\nAl iniciar el c√≥digo el valor que os ha tenido que recuperar, adem√°s de almacenar el objeto en la pesta√±a Environment de RStuido, es:\n[1] 1\nü§î¬øQu√© hemos hecho?\n\nnavalny_fix es el nombre del objeto que almacena la funci√≥n que ha sido asignada. En este caso, a trav√©s de stri_sub, estamos extrayendo una parte del texto navalny_fix . A diferencia del caso anterior, aqu√≠ le estamos pidiendo que, en vez de que recuper un n√∫mero detemrinado de caracteres, escoja todos los que hay comprendidos entre las posiciones que hemos dado a start_v y a end_v. Con esto nos aseguramos que el objeto navalny_fix siempre trabaje en los rangos que nos interesa analizar.\nlength(navalny_fix) es una expresi√≥n que usamos para comprobar cuandos valores existen en nuestro objeto. Es una forma de asegurarnos de que nuestro objeto solo tiene un vector y no es un conjunto de fragmentos de texto. Por eso, al introducirlo, el programa nos devuelve el valor 1 porque solo hay 1 valor dentro de nuestro objeto."
  },
  {
    "objectID": "dataprocessing.html#limpiando-datos",
    "href": "dataprocessing.html#limpiando-datos",
    "title": "Carga y procesamiento de textos",
    "section": "üóëÔ∏è Limpiando datos",
    "text": "üóëÔ∏è Limpiando datos\nYa tenemos nuestro set de datos, nos toca empezar a limpiar antes de tokenizar y empezar a analizar.\n\nPASO 1: Comprobar la estructura del texto\nVamos a abrir un momento nuestro archivo para ver lo que contiene. Aqu√≠ te ense√±o las primeras l√≠neas:\n\n\n2023\nJanuary 12th\nIn my two years behind bars, my only truly original story is the one about the\npsycho. Everything else has been told and described numerous times. If you\nopen any book by a Soviet dissident, there will be endless stories of punishment\ncells, hunger strikes, violence, provocations, lack of medical care. Nothing new.\nBut my story about the psycho is fresh; at least,I‚Äôve never seen or heard\nanything like it\n\nSo, let me give you an idea about the shizo, the place where I sit all the time. It\nis a narrow corridor with cells on either side. The metal doors offer little to no\nsoundproofing, plus there are ventilation holes above the doors, so two people\nsitting in opposite cells can have a conversation without even raising their\nvoices. This is the main reason there has never been anyone in the cell opposite\nmine, or in my entire eight-cell section. I am the only one there, and I have\n\n\nAqu√≠ queda m√°s clara la estructura del texto:\n\nCada entrada del diario empieza con el a√±o en la primera l√≠nea\nDentro de cada a√±o, se dividen por d√≠as las entradas\nLos p√°rrafos est√°n separados por saltos de l√≠nea\nLas entradas est√°n separadas entre s√≠ tambi√©n por una linea en blanco.\n\nA continuaci√≥n lo que vamos a hacer es crear un objeto lista3, en la que cada elemento ser√° una entrada del diario.\n\n\nPASO 2: Conversi√≥n del texto en vectores\nAhora que comprendemos la estructura, vamos a separar el texto en entradas diarias, preservando la estructura de p√°rrafos dentro de cada entrada. Primero dividimos el texto en l√≠neas, donde cada l√≠nea se convierte en un elemento de un vector. Esto nos permite identificar las lineas que contienen fechas y separar las entradas.\n\n# Convertir el texto en un vector de l√≠neas\nlines &lt;- unlist(strsplit(navalny_fix, '\\n')) # \\n indica un salto de l√≠nea\n\nü§î¬øQu√© hemos hecho?\n\nstrsplit() divide el texto por saltos de l√≠nea (\"\\n\"), creando un vector en el que cada l√≠nea es un elemento independiente.\nUsamos unlist() para simplificar la estructura y trabajar con un vector plano.\n\n\nhead(lines) # Veamos las primeras seis l√≠neas de nuestro objeto\n\n[1] \"2023\"                                                                             \n[2] \"January 12th\"                                                                     \n[3] \"In my two years behind bars, my only truly original story is the one about the\"   \n[4] \"psycho. Everything else has been told and described numerous times. If you\"       \n[5] \"open any book by a Soviet dissident, there will be endless stories of punishment\" \n[6] \"cells, hunger strikes, violence, provocations, lack of medical care. Nothing new.\"\n\n\n\n\nPASO 3: Creaci√≥n de √≠ndices para identificar entradas\nEn este paso, vamos a crear los √≠ndices que nos permitir√°n identificar las l√≠neas en el texto que corresponden a cada a√±o y a cada d√≠a. Esto nos ayudar√° a estructurar las entradas en el pr√≥ximo paso.\n\nUtilizando expresiones regulares, vamos a identificar las l√≠neas que contengan s√≥lo el a√±o. Estas l√≠neas marcan el inicio de cada conjunto de entradas anuales\n\n\nyear_indices &lt;- grep(\"^\\\\d{4}$\", lines)\n\nprint(year_indices) # Muestra las l√≠neas que contienen el a√±o\n\n[1]   1 295\n\n\n\nHemos identificado dos l√≠neas que incluyen el a√±o. Ahora haremos lo mismo para las l√≠neas que encuentren el mes y el d√≠a\n\n\nday_indices &lt;- grep(\"^(January|February|March|April|May|June|July|August|September|October|November|December)\\\\s+\\\\d{1,2}(st|nd|rd|th)?$\", lines)\n\nlength(day_indices) # N√∫mero de entradas identificadas\n\n[1] 11\n\n\nü§î¬øQu√© hemos hecho?\n\nyear_indices contiene los √≠ndices de las l√≠neas con los a√±os, es decir, las posiciones donde comienza cada a√±o en el texto. La expresi√≥n regular ^\\\\d{4} busca cuatro d√≠gitos al inicio de la l√≠nea.\nmonth_day_indices contiene los √≠ndices de las l√≠neas con fechas diarias, indicando el inicio de cada d√≠a dentro de los a√±os.\n\n(January|February|...): Busca un mes escrito en ingl√©s.\n\\\\s+: Representa uno o m√°s espacios.\n\\\\d{1,2}(st|nd|rd|th)?$: Busca el d√≠a, que tendr√° uno o dos d√≠gitos y que puede ir seguido de ‚Äúst‚Äù, ‚Äúnd‚Äù, ‚Äúrd‚Äù, o ‚Äúth‚Äù al final de la l√≠nea.\n\n\n\n\nPASO 4: Creaci√≥n de entradas del diario\nAhora que tenemos los √≠ndices para los a√±os y d√≠as, podemos organizar el texto en entradas anidadas: cada a√±o ser√° un grupo principal, y dentro de cada a√±o, cada d√≠a ser√° una entrada individual.\nEl siguiente c√≥digo es un poco tocho, te intento explicar:\n\nCreamos las entradas por a√±o. Para esto utilizamos el objeto year_indices que construimos antes.\nCreamos sublistas dentro de cada a√±o con nuestro objeto month_day_indices.\n\n\n# Dividimos el texto en entradas anidadas (por a√±o y d√≠a)\nyearly_entries &lt;- lapply(\n  seq_along(year_indices), \n  function(i) {\n    start_year &lt;- year_indices[i]\n    end_year &lt;- if (i &lt; length(year_indices))\n      year_indices[i + 1] - 1\n    else\n      length(lines)\n    \n    # Extraemos las l√≠neas correspondientes al a√±o actual\n    year_lines &lt;- lines[start_year:end_year]\n    \n    # Encontrar las entradas diarias dentro del a√±o actual\n    day_indices &lt;- grep(\n      \"^(January|February|March|April|May|June|July|August|September|October|November|December)\\\\s+\\\\d{1,2}(st|nd|rd|th)?$\",\n      year_lines\n      )\n    \n    # Crear una sublista para cada d√≠a dentro del a√±o\n    entries &lt;- lapply(seq_along(day_indices), function(j) {\n      start_day &lt;- day_indices[j]\n      end_day &lt;- if (j &lt; length(day_indices))\n        day_indices[j + 1] - 1\n      else\n        length(year_lines)\n      year_lines[start_day:end_day]\n      })\n  \n  # Devolver una lista con el a√±o y sus entradas diarias\n  list(year = year_lines[1], entries = entries)\n})\n\nü§î¬øQu√© hemos hecho?\n\nCada elemento en yearly_entries es un a√±o completo\n\nprint(yearly_entries[[1]]$year) # primer a√±o\n\n[1] \"2023\"\n\n\nDentro de cada a√±o, entries contiene las entradas diarias como sublistas, donde cada d√≠a es un vector de p√°rrafos\n\nlength(yearly_entries[[1]]$entries)\n\n[1] 9\n\n\n\nPara ello, hemos combinado diferentes funciones en un s√≥lo script:\n\nlapply() sirve para aplicar una funci√≥n a cada elemento de un vector o lista. En nuestro caso, itera sobre cada √≠ndice de year_indices, procesando el texto correspondiente a cada √±o. Genera una lista yearly_indicesdonde cada elemento representa un a√±o y sus entradas diarias. La segunda vez que la empleo es para crear sublistas para cada d√≠a dentro del a√±o, aplic√°ndola sobre day_indices.\nseq_along() genera una secuencia de n√∫meros que corresponden a la longitud de un vector o lista. Lo utilizo para generar una secuencia de √≠ndices a iterar sobre los a√±os y sobre los d√≠as dentro de cada a√±o.\ngrep() busca patrones espec√≠ficos dentro de un vector, tal y como hicimos antes.\nifdentro de lapply()define los l√≠mites de inicio y fin de cada a√±o.\nlist() crea la lista que almacena todas las entradas.\n\n\n\nPASO 5: Asignaci√≥n de nombres a entradas\nDe cara al an√°lisis de datos global, deberemos nombrar cada una de las entradas para que luego seamos capaces de identificarlas.\n\n# Asignar nombres √∫nicos a cada entrada en yearly_entries\nyearly_entries &lt;- lapply(seq_along(yearly_entries), function(i) {\n  year &lt;- yearly_entries[[i]]\n  \n  # Asignar nombres √∫nicos a cada entrada diaria\n  names(year$entries) &lt;- paste0(\"entry_\", i, \"_\", seq_along(year$entries))\n  \n  year  # Devolver el a√±o con las entradas renombradas\n})\n\nü§î¬øQu√© hemos hecho?\n\nHemos aplicado la funci√≥n names()a cada entrada del diario dentro de cada a√±o. Esta funci√≥n asigna un nombre a cada elemento dentro de un objeto. En una tabla de Excel ser√≠a lo equivalente a ponerle un nombre a cada columna de nuestro set de datos.\nPara asignar los nombres de manera autom√°tica y que estos sean √∫nicos empleamos la funci√≥n paste0() que concatena diferentes secuencias de texto, en nuestro caso concatenamos cuatro elementos:\n\nUn texto: \"entry_\".\nEl √≠ndice del a√±o i.\nEl √≠ndice de la entrada, en este caso lo determinamos con la funci√≥n seq_along() que va contando la posici√≥n de cada uno de los elementos entries dentro de la lista year.\n\n\n\n\n[1] \"entry_1_1\" \"entry_1_2\" \"entry_1_3\" \"entry_1_4\" \"entry_1_5\" \"entry_1_6\"\n[7] \"entry_1_7\" \"entry_1_8\" \"entry_1_9\""
  }
]