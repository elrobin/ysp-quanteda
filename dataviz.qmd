---
title: "An√°lisis y visualizaci√≥n de datos"
format: html
toc: TRUE
toc-title: "√çndice"
toc-depth: 4
code-copy: true
editor: visual
---

En este apartado mostramos algunos an√°lisis b√°sicos que se pueden hacer con `quanteda`. Vamos a ir *in crescendo*, de an√°lisis m√°s b√°sicos de frecuencias a t√©cnicas avanzadas de identificaci√≥n de patrones. Aqu√≠ te incluyo lo que tenemos preparado:

1.  An√°lisis de frecuencias global
2.  Nube de palabras (no son muy √∫tiles, pero siempre gustan üòè)
3.  An√°lisis de dispersi√≥n l√©xica
4.  An√°lisis de concordancias (KWIC)
5.  Comparaci√≥n de frecuencias relativas entre t√©rminos
6.  *Topic Modeling*[^1]

[^1]:
    ::: {.callout-tip appearance="simple"}
    El **topic modeling** (modelado de t√≥picos) es una t√©cnica de an√°lisis de texto que identifica temas latentes dentro de un conjunto de documentos. Agrupa palabras que tienden a aparecer juntas, permitiendo descubrir patrones tem√°ticos y estructuras ocultas en el texto.
    :::

```{r, echo=FALSE, message=FALSE}
load(".RData") 
library(quanteda) 
library(readtext) 
library(dplyr) 
library(stringi)
```

## üî¢An√°lisis de frecuencia de datos

El an√°lisis de frecuencia de t√©rminos nos permite identificar las palabras m√°s comunes, facilitando un primer visionado de aquellos posibles temas recurrentes que pudieran ser interesantes de analizar.

```{r}
# Extraer el texto de cada entrada diaria y crear una lista plana 
unified_entries <- unlist(lapply(yearly_entries, function(year) {   sapply(year$entries, function(entry) paste(entry, collapse = " ")) }))    # Crear una matriz de frecuencias de t√©rminos a partir de los textos unificados  
dfm_all <- dfm(tokens(unified_entries))    
# Mostrar las 10 palabras m√°s frecuentes  
top_words <- topfeatures(dfm_all, n = 15)  
top_words
```

**ü§î¬øQu√© hemos hecho?**

-   Hemos unificado todo el texto en el objeto `unified_entries` de manera que ya no distinguimos por a√±o o fecha.

-   Hemos creado una matriz de frecuencias `dfm_all` para todo el conjunto de datos contando las veces que aparece cada palabra.

    ```{r, echo=FALSE}
    print(head(dfm_all))
    ```

Para lograrlo, combinamos varias funciones:

-   `lapply()`: Aplica una funci√≥n a cada elemento de `yearly_entries`, en este caso, iterando sobre cada a√±o en la lista. Dentro de esta funci√≥n, procesamos cada entrada diaria para obtener un texto unificado.

-   `sapply()`: Anidado dentro de `lapply()`, `sapply()` recorre cada entrada diaria de un a√±o espec√≠fico (representada por `year$entries`). Nos permite aplicar `paste()` a cada entrada para unir todas las l√≠neas de la entrada diaria en un solo texto, sin generar sublistas adicionales.

-   `unlist()`: Convierte el resultado de `lapply()` en un vector plano de textos diarios. Esto significa que todos los textos de `yearly_entries` se colocan en una sola estructura de lista sin anidaciones adicionales, facilitando el an√°lisis posterior.

-   `tokens()`: Convierte cada entrada diaria unificada en tokens (es decir, en palabras individuales), lo cual es un paso necesario para aplicar `dfm()`.

-   `dfm()`: Crea una **matriz de frecuencias de t√©rminos** (document-feature matrix) a partir de los tokens generados en el paso anterior. En esta matriz, cada fila representa una entrada diaria y cada columna representa una palabra, con valores que indican la frecuencia de cada palabra en cada entrada.

-   `topfeatures(dfm_all, n = 10)`: Extrae y muestra las 10 palabras m√°s frecuentes en todo el conjunto de datos.

Combinando con el paquete de visualizaci√≥n `ggplot2` podr√≠amos crear un gr√°fico de barras:

```{r}
library(ggplot2)

# Convertir el top 10 en un dataframe
top_words_df <- data.frame(
  word = names(top_words),
  frequency = as.numeric(top_words)
)

# Crear el gr√°fico de barras
ggplot(top_words_df, aes(x = reorder(word, frequency), y = frequency)) +
  geom_bar(stat = "identity", fill = "black") +
  coord_flip() + 
  labs(title = "Top 10 Most Frequent Words",
       x = "Word",
       y = "Frequency") +
  theme_minimal()
```

## üå•Ô∏è Nube de palabras

Una nube de palabras permite identificar visualmente las palabras m√°s frecuentes en el texto. Las palabras m√°s grandes en la nube son las que aparecen con mayor frecuencia, lo cual facilita una comprensi√≥n r√°pida de los t√©rminos m√°s relevantes en el contenido.

Usaremos la matriz de frecuencias de t√©rminos `dfm_all` que creamos previamente para generar la nube de palabras.

```{r}
library(quanteda.textplots)

# Generar la nube de palabras a partir de la matriz de frecuencias
textplot_wordcloud(dfm_all, max_words = 100, color = "orange")

```

**ü§î¬øQu√© hemos hecho?**

-   `textplot_wordcloud()` genera una nube de palabras a partir de la matriz de frecuencias `dfm_all`.

    -   `max_words = 100` limita la visualizaci√≥n a las 100 palabras m√°s frecuentes, para que la nube sea m√°s clara y enfocada en los t√©rminos clave.

    -   `color =` asigna el color de las palabras para una visualizaci√≥n consistente.

## üìçAn√°lisis de dispersi√≥n l√©xica

La dispersi√≥n l√©xica nos permite ver en qu√© partes del texto aparecen ciertas palabras clave. Esto puede ser particularmente √∫til si queremos rastrear c√≥mo cambia el uso de ciertos t√©rminos importantes a lo largo del tiempo o en diferentes contextos.

Usaremos la funci√≥n `textplot_xray()` de `quanteda.textplots` para generar un gr√°fico de dispersi√≥n de las palabras clave en el conjunto de datos. Para ello debermos instalar primero este paquete.

```{r, warning=FALSE}
# Librer√≠a necesaria
library(quanteda.textplots)

# Palabras clave para el an√°lisis de dispersi√≥n l√©xica
keywords <- c("cell", "russia", "people", "shizo")

# Unificar todas las entradas en un solo texto
full_text <- paste(unified_entries, collapse = " ")

# Crear tokens a partir del texto completo
tokens_full_text <- tokens(full_text)

# Generar gr√°fico de dispersi√≥n l√©xica para todo el texto
textplot_xray(kwic(tokens_full_text, pattern = keywords))

```

**ü§î¬øQu√© hemos hecho?**

-   Hemos seleccionado los t√©rminso a analizar bas√°ndonos en el an√°lisis de frecuencias anterior

-   Con la funci√≥n `paste(..., collapse = " ")` hemos unido todas las entradas en el objeto `full_text`, formando un solo texto continuo sin distinguir por a√±o o entrada.

-   Hemos creado los tokens a partir del texto unificado con la funci√≥n `tokens()` para poder trabajar a nivel de palabras

-   La funci√≥n `kwick()` nos sirve para extraer las posiciones de las palabras clave del texto completo.

-   `textplot_xray()` viene del paquete `quanteda.textplots` y nos permite visualizar la dispersi√≥n l√©xica.

## üîç An√°lisis de concordancias (KWIC)

Este an√°lisis muestra cada aparici√≥n de las palabras clave junto con el contexto inmediato, lo que permite estudiar c√≥mo se usan ciertos t√©rminos en diferentes partes del texto.

En este ejemplo vamos a analizar las palabras que preceden y anteceden al t√©rmino `shizo`.

```{r}

# Extraer concordancias de las palabras clave
concordancias <- kwic(tokens_full_text, pattern = "shizo", window = 2)
print(concordancias)

```

**ü§î¬øQu√© hemos hecho?**

-   Aplicamos la funci√≥n `kwic()`para extraer las concordancias de las palabras clave. Es decir, el texto anterior y posterior a estos t√©rminos.

-   El par√°metro `window =` especifica el n√∫mero de palabras que se mostrar√°n antes y despu√©s de cada palabra calve, l oque proporciona un contexto breve que nos permite inspeccionar de manera manual el sentido de cada palabra.

## üß© Topic Modeling

El an√°lisis de temas, o **topic modeling**, es una t√©cnica de procesamiento de lenguaje natural que identifica patrones ocultos en un conjunto de textos. Su objetivo es descubrir **temas latentes** que aparecen recurrentemente, agrupando palabras que suelen aparecer juntas. Cada tema representa un conjunto de palabras que tienden a co-ocurrir, permiti√©ndonos inferir posibles temas o conceptos sin necesidad de leer todo el texto.

El modelo de temas que utilizaremos, llamado **Latent Dirichlet Allocation (LDA)**, funciona tratando cada documento como una combinaci√≥n de varios temas, y cada tema como una combinaci√≥n de palabras. As√≠, LDA genera temas representativos del contenido general de los textos, lo cual es √∫til para identificar patrones y estructura tem√°tica en grandes vol√∫menes de texto.

En primer lugar vamos refinar nuestra matriz de frecuencias `dfm_all` eliminando t√©rminos poco frecuentes

```{r}
dfm_topics <- dfm_trim(dfm_all, min_termfreq = 5)
```

En este caso `min_termfreq = 5`: Elimina t√©rminos que aparecen menos de 5 veces en todo el conjunto de documentos.

Vamos a ver c√≥mo se ha reducido el set de t√©rminos:

```{r}
length(dfm_all) # total de t√©rminos
length(dfm_topics) # total una vez eliminados los poco frecuentes
```

Ahora vamos a usar nuestra nueva matriz de frecuencias `dfm_topics` para aplicar nuestro modelo de `LDA`. Importante, aqu√≠ necesitaremos tener instalado el paquete `tm` y el paquete `topicmodels`.

```{r, message=FALSE}
library(tm)
library(topicmodels)
# Convertir la matriz dfm a formato compatible con LDA
dfm_topics_lda <- convert(dfm_topics, to = "topicmodels")

# Definir el n√∫mero de temas
num_topics <- 3  # Ajusta el n√∫mero de temas seg√∫n el an√°lisis deseado

# Ajustar el modelo LDA
lda_model <- LDA(dfm_topics_lda, k = num_topics, control = list(seed = 1234))

# Obtener las palabras clave de cada tema
terms(lda_model, 10)  # Muestra las 10 palabras m√°s representativas de cada tema

```

**ü§î¬øQu√© hemos hecho?**

-   La funci√≥n `convert()` transforma nuestra matriz de frecuencias en un formato entendible por el paquete `topicmodels` para poder modelarlo.

-   Elegimos el n√∫mero de topics con los que queremos trabajar. Esta parte suele ser una decisi√≥n informada tras hacer varias pruebas. En nuestro caso, me quedar√© con 3 topics.

-   Entreno al modelo con mis datos con la funci√≥n `LDA()`. Para permitir reproducibilidad del an√°lisis utilizo el par√°metro `control=` para establecer una semilla para el generador de n√∫meros aleatorios que emplea el algoritmo.
